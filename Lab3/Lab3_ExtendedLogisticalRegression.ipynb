{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6ea153",
   "metadata": {},
   "source": [
    "# Lab Assignment Three: Extending Logistic Regression\n",
    "\n",
    "## ***** Catherine Magee, Morgan Mote, Luv Patel *****\n",
    "\n",
    "### In this lab, you will compare the performance of logistic regression optimization programmed in scikit-learn and via your own implementation. You will also modify the optimization procedure for logistic regression. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94076c02",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Select a dataset identically to the way you selected for the lab one (i.e., table data and you must have a mix of numeric and categorical variables). You are not required to use the same dataset that you used in the past, but you are encouraged. You must identify a classification task from the dataset that contains three or more classes to predict. That is it cannot be a binary classification; it must be multi-class prediction. \n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "### Preparation and Overview (3 points total)\n",
    "- [2 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? As in previous labs, also detail how good the classifier needs to perform in order to be useful. \n",
    "- [.5 points] (mostly the same processes as from previous labs) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). Provide a breakdown of the variables after preprocessing (such as the mean, std, etc. for all variables, including numeric and categorical). \n",
    "- [.5 points] Divide your data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  \n",
    "\n",
    "### Modeling (5 points total)\n",
    "- The implementation of logistic regression must be written only from the examples given to you by the instructor. No credit will be assigned to teams that copy implementations from another source, regardless of if the code is properly cited. \n",
    "- [2 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template developed by the instructor in the course. You should add the following functionality to the logistic regression classifier:\n",
    "  - Ability to choose optimization technique when class is instantiated: either steepest ascent, stochastic gradient ascent, and {Newton's method/Quasi Newton methods}. \n",
    "  - Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1 and L2 regularization). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  \n",
    "- [1.5 points] Train your classifier to achieve good generalization performance. That is, adjust the optimization technique and the value of the regularization term(s) \"C\" to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "- [1.5 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. Discuss the results. \n",
    "\n",
    "### Deployment (1 points total)\n",
    "- Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?\n",
    "\n",
    "### Exceptional Work (1 points total)\n",
    "- You have free reign to provide additional analyses. One idea: Update the code to use either \"one-versus-all\" or \"one-versus-one\" extensions of binary to multi-class classification. \n",
    "\n",
    "- Required for 7000 level students: Choose ONE of the following:\n",
    "\n",
    "  - Option One: Implement an optimization technique for logistic regression using mean square error as your objective function (instead of maximum likelihood). Derive the gradient updates for the Hessian and use Newton's method to update the values of \"w\". Then answer, which process do you prefer: maximum likelihood OR minimum mean-squared error? \n",
    "  - Option Two: Implement the BFGS algorithm from scratch to optimize logistic regression. That is, use BFGS without the use of an external package (for example, do not use SciPy). Compare your performance accuracy and runtime to the BFGS implementation in SciPy (that we used in lecture). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "import textwrap\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import subprocess\n",
    "import copy\n",
    "from typing import Optional\n",
    "\n",
    "# Third-Party Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import bokeh\n",
    "import missingno as mn\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Sci-py Imports\n",
    "import scipy\n",
    "from scipy.special import expit\n",
    "\n",
    "# Imblearn Imports\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Skimage Imports\n",
    "import skimage\n",
    "\n",
    "# Sklearn Imports\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    LabelEncoder,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "# Warning Filters\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"fastai\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "#Random number generator seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Import\n",
    "#CSV file is in the same directory as Python script\n",
    "file_path = \"COVID-19_cases_plus_census.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "cases = pd.read_csv(file_path)\n",
    "\n",
    "# Now, 'cases' contains the data from the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfa7b2",
   "metadata": {},
   "source": [
    "## Business Understanding and Problem Solving\n",
    "\n",
    "Here is a detailed description of the task and what business-case or use-case it is designed to solve (or designed to investigate). We have detailed exactly what the classification task is and what parties would be interested in the results. We should consider the example, would the model be deployed or used mostly for offline analysis? As in previous labs, we also need to detail how well the classifier needs to perform in order to be useful.\n",
    "\n",
    "We retrieved out dataset from the following source: https://console.cloud.google.com/marketplace/details/usafacts-public-data/covid19-us-cases?filter=solution-type:dataset&filter=category:covid19&id=3eaff9c5-fbaf-47bb-a441-89db1e1395ab&project=still-nebula-398202 [1]\n",
    "\n",
    "### ***COPIED FROM FIRST PROJECT PLEASE ENHANCE ACCORDINGLY FOR THIS PROJECT***\n",
    "\n",
    "This exploratory analysis focuses on COVID 19 data pulled from the Google Cloud Public Datasets Program. More Specifically, this dataset focuses on a census report in Texas that was collected during the pandemic in 2021. The purpose of this dataset is to discover which counties in Texas are at highest risk for positive COVID 19 cases.\n",
    "\n",
    "This analysis is essential to flattening the curve or slowing the spread of postive COVID 19 cases because it surfaces not only the counties that are at the highest risk but also which demographic in that county needs the most assistance. Therefore this analysis can provide useful information to third parties such as hospitals in their distribution of personal protective equipment (PPE) such as masks.\n",
    "\n",
    "The COVID-19 pandemic had a major impact on every facet of the human experience for people all over the world. Businesses closed while people lost their jobs, homes, and loved ones. Through the significant human impact, data collection has allowed analysts to shed light on exactly how the pandemic affected businesses and economies worldwide. On a regulatory front, there were many inconsistencies in public health regulations and guidelines between federal and state agencies. These disparities fueled public health concerns and debates which contributed to an increased sense of confusion and lack of effectiveness of the measures put in place to protect the public from COVID-19 infection. The data was collected to provide an accurate view of the affects of a pandemic in the US so that we may be better prepared to face a pandemic in the future.\n",
    "\n",
    "Through our analysis, we have revealed that some areas of the country have less deaths overall but higher mortality rates from confirmed COVID-19 cases due to many factors such as local population size and density, proximity to an urban area, and many other aspects will be explored. Some areas with a low population have experienced a higher deaths-per-thousand count than large urban areas with a higher number of deaths but a lower count of deaths-per-thousand. The goal of this analysis is to predict which counties will be at the highest risk with the lowest amount of resources in the future. Based on these notions and within a certain degree of accuracy, it may be possible to predict the development in a region given comparable data from other regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c248740",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826451ae",
   "metadata": {},
   "source": [
    "### Feature Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16414278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_list_of_features():\n",
    "    columns = df.columns.to_list()\n",
    "    features = \"Features: \"\n",
    "    for i, col in enumerate(columns):\n",
    "        if i > 0:\n",
    "            features += f', \"{col}\"'\n",
    "        else:\n",
    "            features += f'\"{col}\"'\n",
    "    print(features)\n",
    "\n",
    "print_df_list_of_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e226c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_shape():\n",
    "    rows, cols = df.shape\n",
    "    print(f\"~ Dataframe Size ~\\nRows: {rows}\\nCoumns: {cols}\")\n",
    "\n",
    "print_df_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with confirmed_cases > 0\n",
    "cases = cases[cases['confirmed_cases'] > 0]\n",
    "\n",
    "# Calculate rates per 1000 people \n",
    "# Rate calculation added as new attributes\n",
    "cases['cases_per_1000'] = (cases['confirmed_cases'] / cases['total_pop']) * 1000\n",
    "cases['deaths_per_1000'] = (cases['deaths'] / cases['total_pop']) * 1000\n",
    "cases['death_per_case'] = cases['deaths'] / cases['confirmed_cases']\n",
    "\n",
    "# Sort the DataFrame by confirmed_cases in descending order\n",
    "cases = cases.sort_values(by='confirmed_cases', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe1d5f",
   "metadata": {},
   "source": [
    "### Prepare Feature Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1863f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select important attributes\n",
    "selected_columns = ['county_name', 'state', 'confirmed_cases', 'deaths', 'total_pop', 'median_income', \n",
    "                    'median_age', 'cases_per_1000', 'deaths_per_1000', 'death_per_case']\n",
    "cases = cases[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a26f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_col_counts():\n",
    "    cols = ['county_name', 'state', 'confirmed_cases', 'deaths', 'total_pop', 'median_income', \n",
    "                    'median_age', 'cases_per_1000', 'deaths_per_1000', 'death_per_case']\n",
    "    print(f\"{' Value Counts '.center(40, '*')}\\n\")\n",
    "    for col in cols:\n",
    "        print(df[col].value_counts(), \"\\n\")\n",
    "    print(f\"{''.center(40, '*')}\")\n",
    "\n",
    "get_feature_col_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ccb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names in the 'cases' DataFrame\n",
    "print(\"These are the cases in the dataframe. \\n\" + str(cases.columns)+ \"\\n\" + \"\\n\")\n",
    "\n",
    "# Print the first few rows in the resulting DataFrame\n",
    "print(cases.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc43be",
   "metadata": {},
   "source": [
    "### Validating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6784ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = cases.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(str(missing_values)+ \"\\n\")\n",
    "\n",
    "print(\"These are the columns with missing data:\" + str(cases.columns[cases.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57976b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate data\n",
    "def check_for_duplicate_entries():\n",
    "    return df.duplicated().any()\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Duplicates Found\"\n",
    ") if check_for_duplicate_entries() is True else print(\"No Duplicates Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbeb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "def visualize_missing_data(title):\n",
    "    mn.matrix(df.sort_values(by=[\" ########## \"]), color=(.4, .75, 1)) # replace with category from our data set\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "visualize_missing_data(\"Missing Data Visualized (Before Imputation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c908b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using K-Nearest Neighbor for the column BMI\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "bmi_imputed_vals = imputer.fit_transform(df[\" ########## \"].to_numpy().reshape(-1, 1)) # replace with same category from above fuction\n",
    "df[\" ########## \"] = bmi_imputed_vals # replace with same category from above fuction\n",
    "\n",
    "# Show that missing values were imputed\n",
    "visualize_missing_data(\"Missing Data Visualized (After Imputation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4508f27",
   "metadata": {},
   "source": [
    "### Identify Categorical Feature Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_feature_categories():\n",
    "    print(\"%s\\n\" % format(\" Unique Categories \".center(50, \"*\")))\n",
    "\n",
    "    selected_cols = df.select_dtypes(exclude=[np.number]).columns.to_list()\n",
    "    selected_cols = [c for c in selected_cols]\n",
    "\n",
    "    for col in selected_cols:\n",
    "        if not isinstance(df[col].values, float):\n",
    "            unique_vals = pd.unique(df[col].values)\n",
    "            print(\n",
    "                col, ', '.join([v for v in unique_vals if not isinstance(v, float)]),\n",
    "                sep=\": \"\n",
    "            )\n",
    "            print()\n",
    "\n",
    "    print(\"%s\" % format(\"\".center(50, \"*\")))\n",
    "\n",
    "unique_feature_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4e702",
   "metadata": {},
   "source": [
    "### Identify Continuous Feature Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ \"median_age\", \"confirmed_cases\", \"deaths_per_case\"].describe(include=[np.number]).round(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b64bf6",
   "metadata": {},
   "source": [
    "### View Data Based on Gender and Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ebde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_gender_and_age():\n",
    "    age = df[\"age\"].values\n",
    "    bins = [age for age in range(0, 100, 10)]  # Generator for [0, ..., 90]\n",
    "\n",
    "    age_binned = pd.cut(age, bins, labels=False)\n",
    "\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df[\"age group\"] = age_binned\n",
    "\n",
    "    tmp_df.rename(\n",
    "        columns={\n",
    "            \"median_age\": \"median age\",\n",
    "            \"total_pop\": \"total population\",\n",
    "            \"confirmed_cases\": \"confirmed cases\",\n",
    "            \"median_income\": \"median income\",\n",
    "            \"deaths_per_1000\": \"deaths per 1000\"\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    grouped = (\n",
    "        tmp_df.groupby([\"gender\", \"age group\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"median age\": \"mean\",\n",
    "                \"total population\": \"sum\",\n",
    "                \"confirmed cases\": \"sum\",\n",
    "                \"median income\": \"mean\",\n",
    "                \"deaths per 1000\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .round(1)\n",
    "    )\n",
    "    display(HTML(grouped.to_html()))\n",
    "\n",
    "\n",
    "group_gender_and_age()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d22272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_gender_counts():\n",
    "    gender_counts = df[\"gender\"].value_counts().to_dict()\n",
    "    tmp_df = pd.DataFrame.from_dict(gender_counts, orient='index', columns=['count'])\n",
    "    tmp_df = tmp_df.rename({0: \"Male\", 1: \"Female\"})\n",
    "    display(HTML(tmp_df.to_html()))\n",
    "\n",
    "group_gender_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9be33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"gender\", \"deaths_per_case\"]].groupby(\"gender\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa432bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('deaths_per_case').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b21b02",
   "metadata": {},
   "source": [
    "### Define Target Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10910136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable from the rest of the features\n",
    "target = df[\"deaths_per_case\"]\n",
    "features = df.drop(\"deaths_per_case\", axis=1)\n",
    "\n",
    "continuous_features = [\n",
    "    \"median_age\",\n",
    "    \"confirmed_cases\",\n",
    "    \"deaths_per_case\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \" aaaa \",\n",
    "    \" bbbb \",\n",
    "    \" cccc \",\n",
    "    \" dddd \",\n",
    "    \" eeee \",\n",
    "    \" ffff \", # replace with actual category features\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e82eaf",
   "metadata": {},
   "source": [
    "### Normalize Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ce5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[continuous_features] = scaler.fit_transform(features[continuous_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5356371",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b426a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([0, 1], np.bincount(target))\n",
    "plt.xticks([0, 1], ['No Confirmation', 'Confirmed'])\n",
    "plt.title('Class Imbalance For COVID-19 Confirmed Cases Data') \n",
    "plt.show()\n",
    "\n",
    "# please change the attribute for the class imbalance section if it doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31591a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_for_class_imbalance():\n",
    "    \"\"\"\n",
    "    Use SMOTE for balancing the dataset to deal with the relatively\n",
    "    small number of unconfirmed cases. IF THERE EVEN ARE ANY, MIGHT NEED TO SELECT A DIFFERENT ATTRIBUTE!!!***************\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    features[categorical_features] = features[categorical_features].apply(\n",
    "        lambda ftr: encoder.fit_transform(ftr)\n",
    "    )\n",
    "    oversample_smote = SMOTE(random_state=123)\n",
    "    x_resample, y_resample = oversample_smote.fit_resample(features, target)\n",
    "    return x_resample, y_resample\n",
    "\n",
    "\n",
    "x_resample, y_resample = resample_for_class_imbalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([0, 1], np.bincount(y_resample))\n",
    "plt.xticks([0, 1], ['No Confirmation', 'Confirmed'])\n",
    "plt.title(\"Class Balance After Using SMOTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_resample.shape, y_resample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479a9f6",
   "metadata": {},
   "source": [
    "### View Finalized Preprocessed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeac6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_resample.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38392fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resample.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb35ed1",
   "metadata": {},
   "source": [
    "### Split Into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd310459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    # features, target\n",
    "    x_resample,\n",
    "    y_resample,\n",
    "    test_size=0.2,\n",
    "    random_state=123, # refer to import cell\n",
    "    stratify=y_resample,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944503a",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab89dde",
   "metadata": {},
   "source": [
    "### Initial Logistic Regression Using Sklearn \n",
    "\n",
    "[2] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the preprocessor and the linear regression model into a pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[(\"regressor\", LogisticRegression())]\n",
    ")\n",
    "pipeline.fit(x_train, y_train)\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model on the testing data\n",
    "score = pipeline.score(x_test, y_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361e358",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression\n",
    "\n",
    "[3] https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Binary Logistic Regression\n",
    "class BinaryLogisticRegressionBase:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with\n",
    "        # sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Base Binary Logistic Regression Object, Not Trainable\"\n",
    "\n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1 / (1 + np.exp(-theta))\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "    \n",
    "    def _add_bias(self, X):\n",
    "        return self._add_intercept(X)\n",
    "\n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de26467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Binary Logistic Regression Object\"\n",
    "\n",
    "    def _get_gradient(self, X, y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape)  # set gradient to zero\n",
    "        for (xi, yi) in zip(X, y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi, add_intercept=False)) * xi\n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape)\n",
    "\n",
    "        return gradient / float(len(y))\n",
    "\n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        # init weight vector to zeros\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta  # multiply by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_logistic_regression(**kwargs):\n",
    "    blr = BinaryLogisticRegression(**kwargs)\n",
    "    blr.fit(x_train, y_train)\n",
    "    y_pred = blr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {blr.w_.shape}\")\n",
    "    print(blr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_binary_logistic_regression(eta=0.1, iterations=24)\n",
    "run_binary_logistic_regression(eta=0.1, iterations=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f5b44",
   "metadata": {},
   "source": [
    "### Line Search Logistic Regression\n",
    "\n",
    "[4] https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "\n",
    "    # define custom line search for problem\n",
    "    def __init__(self, line_iters=0.0, C=0.005, **kwargs):\n",
    "        self.line_iters = line_iters\n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwargs)  # call parent initializer\n",
    "\n",
    "    # this defines the function with the first input to be optimized\n",
    "    # therefore eta will be optimized, with all inputs constant\n",
    "    @staticmethod\n",
    "    def objective_function(eta, X, y, w, grad, C):\n",
    "        wnew = w - grad * eta\n",
    "        g = expit(X @ wnew)\n",
    "        # the line search is looking for minimization, so take the negative of l(w)\n",
    "        return (\n",
    "            -np.sum(np.ma.log(g[y == 1]))\n",
    "            - np.ma.sum(np.log(1 - g[y == 0]))\n",
    "            + C * sum(wnew**2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb, y)\n",
    "            # minimization is in opposite direction\n",
    "\n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {\"maxiter\": self.line_iters}  # unclear exactly what this should be\n",
    "            res = scipy.optimize.minimize_scalar(\n",
    "                self.objective_function,  # objective function to optimize\n",
    "                bounds=(0, self.eta * 10),  # bounds to optimize\n",
    "                args=(\n",
    "                    Xb,\n",
    "                    y,\n",
    "                    self.w_,\n",
    "                    gradient,\n",
    "                    self.C,\n",
    "                ),  # additional argument for objective function\n",
    "                method=\"bounded\",  # bounded optimization for speed\n",
    "                options=opts,\n",
    "            )  # set max iterations\n",
    "\n",
    "            eta = res.x  # get optimal learning rate\n",
    "            self.w_ -= gradient * eta  # set new function values\n",
    "            # subtract to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_line_search_logistic_regression(**kwargs):\n",
    "    lslr = LineSearchLogisticRegression(**kwargs)\n",
    "    lslr.fit(x_train, y_train)\n",
    "    y_pred = lslr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {lslr.w_.shape}\")\n",
    "    print(lslr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_line_search_logistic_regression(eta=0.1, iterations=24, C=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42d9f5",
   "metadata": {},
   "source": [
    "### Vectorized Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e655f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Vector Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Vector Binary Logistic Regression Object\"\n",
    "\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta)) \n",
    "\n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self, X, y):\n",
    "        ydiff = (\n",
    "            y - self.predict_proba(X, add_intercept=False).ravel()\n",
    "        )  # get y difference\n",
    "        gradient = np.mean(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "\n",
    "        return gradient.reshape(self.w_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectorized_logistic_regression(**kwargs):\n",
    "    vlr = VectorBinaryLogisticRegression(**kwargs)\n",
    "    vlr.fit(x_train, y_train)\n",
    "    y_pred = vlr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {vlr.w_.shape}\")\n",
    "    print(vlr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84847740",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_vectorized_logistic_regression(eta=0.1, iterations=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb412e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_vectorized_logistic_regression(eta=0.1, iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff008e",
   "metadata": {},
   "source": [
    "### Multi-Class (One-Versus-All) Logistic Regression\n",
    "\n",
    "[5] https://github.com/mGalarnyk/DSGO_IntroductionScikitLearn/blob/main/notebooks/LogisticOneVsAll.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c00321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"MultiClass Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained MultiClass Logistic Regression Object\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y)  # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []  # will fill this array with binary classifiers\n",
    "\n",
    "        for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "            y_binary = y == yval  # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta, self.iters)\n",
    "            blr.fit(X, y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(\n",
    "                blr.predict_proba(X)\n",
    "            )  # get probability for each classifier\n",
    "\n",
    "        return np.hstack(probs)  # make into single matrix\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.unique_[\n",
    "            np.argmax(self.predict_proba(X), axis=1)\n",
    "        ]  # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdae317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiclass_logistic_regression(**kwargs):\n",
    "    mlr = MCLogisticRegression(**kwargs)\n",
    "    mlr.fit(x_train, y_train)\n",
    "    y_pred = mlr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {mlr.w_.shape}\")\n",
    "    print(mlr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_logistic_regression(eta=0.1, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bf4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiclass_logistic_regression(eta=0.1, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_own_logistic_regression_model_with_scikit_version():\n",
    "    lr_model = LogisticRegression(solver='liblinear')\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    y_pred = lr_model.predict(x_test)\n",
    "    print(\"***** Scikit-Learn Logistic Regression Model *****\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "compare_own_logistic_regression_model_with_scikit_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31c714",
   "metadata": {},
   "source": [
    "### Logistic Regression Class Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59501a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(lr, Xin, y, title=\"\"):\n",
    "    new_x = Xin[['confirmed_cases', 'deaths_per_case']]\n",
    "    Xb = copy.deepcopy(new_x)\n",
    "    lr.fit(Xb, y)  # train only on two features\n",
    "\n",
    "    h = 0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb.avg_glucose_level.min() - 1, Xb.avg_glucose_level.max() + 1\n",
    "    y_min, y_max = Xb.bmi.min() - 1, Xb.bmi.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb.avg_glucose_level, Xb.bmi, c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel(\"Confirmed Cases\")\n",
    "    plt.ylabel(\"Deaths per Case\")\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "mc_lr_model = MCLogisticRegression(eta=0.1, iterations=500)\n",
    "plot_decision_boundaries(\n",
    "    mc_lr_model, x_train, y_train, \"Logistic Regression Class Boundaries (User-Defined)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ed680",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(solver='liblinear')\n",
    "plot_decision_boundaries(\n",
    "    lr_model, x_train, y_train, \"Logistic Regression Class Boundaries (Sklearn)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834e35b",
   "metadata": {},
   "source": [
    "### Regularized Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwargs):\n",
    "        # need to add to the original initializer\n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwargs)  # call parent initializer\n",
    "\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Regularized Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Regularized Binary Logistic Logistic Regression Object\"\n",
    "\n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self, X, y, reg=\"l1+l2\"):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X, y)\n",
    "\n",
    "\n",
    "        if reg == \"l1\":\n",
    "            # add in regularization (to all except bias term)\n",
    "            gradient[1:] += -2 * np.sign(self.w_[1:]) * self.C\n",
    "            return gradient\n",
    "        \n",
    "        elif reg == \"l2\":\n",
    "            # add in regularization (to all except bias term)\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "            return gradient\n",
    "        \n",
    "        if reg == \"l1+l2\":\n",
    "            # add in regularization (to all except bias term)\n",
    "            l1gradient = -2 * np.sign(self.w_[1:]) * self.C\n",
    "            l2gradient = -2 * self.w_[1:] * self.C\n",
    "            gradient[1:] += l1gradient\n",
    "            gradient[1:] += l2gradient\n",
    "            return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLogisticRegression(MCLogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwargs):\n",
    "        # need to add to the original initializer\n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwargs)  # call parent initializer\n",
    "    \n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Regularized Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Regularized Logistic Regression Object\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y)  # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []  # will fill this array with binary classifiers\n",
    "\n",
    "        for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "            y_binary = y == yval  # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            # now this has regularization built into it\n",
    "            blr = RegularizedBinaryLogisticRegression(\n",
    "                eta=self.eta, iterations=self.iters, C=self.C\n",
    "            )\n",
    "            blr.fit(X, y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a641f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regularized_binary_logistic_regression(**kwargs):\n",
    "    rblr = RegularizedBinaryLogisticRegression(**kwargs)\n",
    "    rblr.fit(x_train, y_train)\n",
    "    y_pred = rblr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {rblr.w_.shape}\")\n",
    "    print(rblr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_regularized_binary_logistic_regression(C=0.0, eta=0.1, iterations=1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regularized_logistic_regression(**kwargs):\n",
    "    rlr = RegularizedLogisticRegression(**kwargs)\n",
    "    rlr.fit(x_train, y_train)\n",
    "    y_pred = rlr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {rlr.w_.shape}\")\n",
    "    print(rlr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_regularized_logistic_regression(C=0.0, eta=0.1, iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ec716",
   "metadata": {},
   "source": [
    "### Linear Regression Using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19299982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression:\n",
    "    def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Stochastic Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Stochastic Logistic Regression Object\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self, X, y):\n",
    "        idx = np.random.randint(0, len(y)) # Grab random instance\n",
    "        # idx = int(np.random.rand() * len(y))  # grab random instance\n",
    "\n",
    "        y_val = y[idx]\n",
    "        x_val = self.predict_proba(X[idx], add_bias=False)\n",
    "        ydiff = y_val - x_val  # get y difference (now scalar)\n",
    "\n",
    "        gradient = (\n",
    "            X[idx] * ydiff[:, np.newaxis]\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for ctr, _ in enumerate(range(self.iters)):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta  # multiply by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stochastic_gradient_descent(**kwargs):\n",
    "    slr = StochasticLogisticRegression(**kwargs)\n",
    "    slr.fit(x_train, y_train)\n",
    "    y_pred = slr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {slr.w_.shape}\")\n",
    "    print(slr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_stochastic_gradient_descent(C=0.0001, eta=0.005, iterations=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e5de7",
   "metadata": {},
   "source": [
    "### Steepest Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestGradientDescentLogisticRegression:\n",
    "    def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Steepest Gradient Descent Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Steepest Gradient Descent Logistic Regression Object\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self, X, y):\n",
    "        idx = np.random.randint(0, len(y)) # Grab random instance\n",
    "        # idx = int(np.random.rand() * len(y))  # grab random instance\n",
    "\n",
    "        y_val = y[idx]\n",
    "        x_val = self.predict_proba(X[idx], add_bias=False)\n",
    "        ydiff = y_val - x_val  # get y difference (now scalar)\n",
    "\n",
    "        gradient = (\n",
    "            X[idx] * ydiff[:, np.newaxis]\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        # self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "        self.w_ = np.zeros(num_features)\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for ctr, _ in enumerate(range(self.iters)):\n",
    "            # gradient = self._get_gradient(Xb, y)\n",
    "            # self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "\n",
    "            # Calculate the predicted values\n",
    "            z = np.dot(Xb, self.w_)\n",
    "            h = 1 / (1 + np.exp(-z))\n",
    "            # Calculate the error\n",
    "            error = h - y\n",
    "            # Calculate the gradient\n",
    "            gradient = np.dot(Xb.T, error) / num_samples\n",
    "            # Update the parameters\n",
    "            self.w_ = self.w_ - self.eta * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b90243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steepest_gradient_descent(**kwargs):\n",
    "    sgdlr = SteepestGradientDescentLogisticRegression(**kwargs)\n",
    "    sgdlr.fit(x_train, y_train)\n",
    "    y_pred = sgdlr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {sgdlr.w_.shape}\")\n",
    "    print(sgdlr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_steepest_gradient_descent(C=0.0001, eta=0.005, iterations=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4519b9",
   "metadata": {},
   "source": [
    "### Hessian Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianLogisticRegression:\n",
    "    def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Hessian Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Hessian Binary Logistic Regression Object\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self, X, y):\n",
    "        g = self.predict_proba(\n",
    "            X, add_bias=False\n",
    "        ).ravel()  # get sigmoid value for all classes\n",
    "        hessian = (\n",
    "            X.T @ np.diag(g * (1 - g)) @ X - 2 * self.C\n",
    "        )  # calculate the hessian\n",
    "\n",
    "        ydiff = y - g  # get y difference\n",
    "        gradient = np.sum(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for ctr, _ in enumerate(range(self.iters)):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta  # multiply by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hessian_binary_logistic_regression(**kwargs):\n",
    "    hblr = HessianLogisticRegression(**kwargs)\n",
    "    hblr.fit(x_train, y_train)\n",
    "    y_pred = hblr.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {hblr.w_.shape}\")\n",
    "    print(hblr)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_hessian_binary_logistic_regression(C=0.0001, eta=0.005, iterations=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007c493",
   "metadata": {},
   "source": [
    "### Hessian Binary Logistic Regression (Rank Two Approximation) From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianLogisticRegressionRankTwo:\n",
    "    def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Hessian Binary Logistic Regression BFGS Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Hessian Binary Logistic Regression BFGS Object\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_function(w, X, y, C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return (\n",
    "            -np.sum(np.ma.log(g[y == 1]))\n",
    "            - np.sum(np.ma.log(1 - g[y == 0]))\n",
    "            + C * sum(w**2)\n",
    "        )\n",
    "        # -np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w, X, y, C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y - g  # get y difference\n",
    "        gradient = np.mean(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def backtracking_line_search(f, grad_f, x, p, *args, alpha=1, beta=0.5):\n",
    "        \"\"\"Perform a backtracking line search to find a step size that\n",
    "        satisfies the Armijo condition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : callable\n",
    "            Objective function to minimize.\n",
    "        grad_f : callable\n",
    "            Gradient function of the objective function.\n",
    "        x : array-like\n",
    "            Current point in the search space.\n",
    "        p : array-like\n",
    "            Search direction.\n",
    "        args : tuple\n",
    "            Extra arguments to pass to the objective and gradient functions.\n",
    "        alpha : float, optional\n",
    "            Initial step size.\n",
    "        beta : float, optional\n",
    "            Factor by which to decrease the step size at each iteration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Step size that satisfies the Armijo condition.\n",
    "        \"\"\"\n",
    "        while f(x + alpha * p, *args) > f(x, *args) + alpha * beta * np.dot(\n",
    "            grad_f(x, *args).T, p\n",
    "        ):\n",
    "            alpha *= beta\n",
    "        return alpha\n",
    "\n",
    "    def bfgs(\n",
    "        self, f, x0, grad_f, args=(), epsilon=1e-5, max_iterations=10, verbose=False\n",
    "    ):\n",
    "        \"\"\"Minimize a function using the BFGS algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : callable\n",
    "            Objective function to minimize.\n",
    "        x0 : array-like\n",
    "            Initial guess for the solution.\n",
    "        grad_f : callable\n",
    "            Gradient function of the objective function.\n",
    "        args : tuple, optional\n",
    "            Extra arguments to pass to the objective and gradient functions.\n",
    "        epsilon : float, optional\n",
    "            Tolerance for the norm of the gradient.\n",
    "        max_iterations : int, optional\n",
    "            Maximum number of iterations to perform.\n",
    "        verbose : bool, optional\n",
    "            Whether to print information about the optimization process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : ndarray\n",
    "            The solution found by the optimization algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        x = x0\n",
    "        fx = f(x, *args)\n",
    "        grad_fx = grad_f(x, *args)\n",
    "        H = np.eye(x0.shape[0])\n",
    "\n",
    "        for i in range(max_iterations):\n",
    "            if np.linalg.norm(grad_fx) < epsilon:\n",
    "                if verbose:\n",
    "                    print(f\"Optimization converged in {i} iterations.\")\n",
    "                return x\n",
    "\n",
    "            p = -np.dot(H, grad_fx)\n",
    "            alpha = self.backtracking_line_search(f, grad_f, x, p, *args)\n",
    "            x_next = x + alpha * p\n",
    "            s = x_next - x\n",
    "            y = grad_f(x_next, *args) - grad_fx\n",
    "            rho = 1 / np.dot(y.T, s)\n",
    "            H = (np.eye(x.shape[0]) - rho * np.outer(s, y)) @ H @ (\n",
    "                np.eye(x.shape[0]) - rho * np.outer(y, s)\n",
    "            ) + rho * np.outer(s, s)\n",
    "            x = x_next\n",
    "            fx = f(x, *args)\n",
    "            grad_fx = grad_f(x, *args)\n",
    "\n",
    "            if verbose and i % 10 == 0:\n",
    "                print(\n",
    "                    f\"Iteration {i}: f(x)={fx}, ||grad f(x)||={np.linalg.norm(grad_fx)}\"\n",
    "                )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Optimization stopped after {max_iterations} iterations.\")\n",
    "        return x\n",
    "\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        # Run bfgs here....\n",
    "        self.w_ = self.bfgs(\n",
    "            self.objective_function,\n",
    "            np.zeros((num_features,)),\n",
    "            self.objective_gradient,\n",
    "            args=(Xb, y, self.C),\n",
    "            epsilon=1e-3,\n",
    "            max_iterations=self.iters,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        self.w_ = self.w_.reshape((num_features, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4caa44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hessian_binary_logistic_regression_rank_two(**kwargs):\n",
    "    hblr2 = HessianLogisticRegressionRankTwo(**kwargs)\n",
    "    hblr2.fit(x_train, y_train)\n",
    "    y_pred = hblr2.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {hblr2.w_.shape}\")\n",
    "    print(hblr2)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_hessian_binary_logistic_regression_rank_two(C=0.0001, eta=0.005, iterations=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31623efe",
   "metadata": {},
   "source": [
    "### Hessian Binary Logistic Regression Rank Two (Scipy Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc670d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianLogisticRegressionBFGS:\n",
    "    def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "\n",
    "    def __str__(self):\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Hessian Binary Logistic Regression BFGS Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        else:\n",
    "            return \"Untrained Hessian Binary Logistic Regression BFGS Object\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_function(w, X, y, C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return (\n",
    "            -np.sum(np.ma.log(g[y == 1]))\n",
    "            - np.sum(np.ma.log(1 - g[y == 0]))\n",
    "            + C * sum(w**2)\n",
    "        )\n",
    "        # -np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w, X, y, C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y - g  # get y difference\n",
    "        gradient = np.mean(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = scipy.optimize.fmin_bfgs(\n",
    "            self.objective_function,  # what to optimize\n",
    "            np.zeros((num_features, 1)),  # starting point\n",
    "            fprime=self.objective_gradient,  # gradient function\n",
    "            args=(\n",
    "                Xb,\n",
    "                y,\n",
    "                self.C,\n",
    "            ),  # extra args for gradient and objective function\n",
    "            gtol=1e-03,  # stopping criteria for gradient, |v_k|\n",
    "            maxiter=self.iters,  # stopping criteria iterations\n",
    "            disp=False,\n",
    "        )\n",
    "\n",
    "        self.w_ = self.w_.reshape((num_features, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaa380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hessian_binary_logistic_regression_bfgs(**kwargs):\n",
    "    hblr_bfgs = HessianLogisticRegressionBFGS(**kwargs)\n",
    "    hblr_bfgs.fit(x_train, y_train)\n",
    "    y_pred = hblr_bfgs.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {hblr_bfgs.w_.shape}\")\n",
    "    print(hblr_bfgs)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_hessian_binary_logistic_regression_bfgs(C=0.001, eta=0.005, iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc93a8",
   "metadata": {},
   "source": [
    "### BFGS and Newton's Method for Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=HessianLogisticRegressionBFGS):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212777a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hessian_binary_logistic_regression_bfgs_and_newton(**kwargs):\n",
    "    hblr_bfgs = MultiClassLogisticRegression(**kwargs)\n",
    "    hblr_bfgs.fit(x_train, y_train)\n",
    "    y_pred = hblr_bfgs.predict(x_test)\n",
    "    \n",
    "    print(f\"Shape: {hblr_bfgs.w_.shape}\")\n",
    "    print(hblr_bfgs)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "run_hessian_binary_logistic_regression_bfgs(C=0.001, eta=0.005, iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdff223",
   "metadata": {},
   "source": [
    "### Custom Logistic Regression Model Selector\n",
    "\n",
    "[6] https://www.kaggle.com/code/junkal/selecting-the-best-regression-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelector:\n",
    "    models = [\n",
    "        \"Binary\",\n",
    "        \"Line Search\",\n",
    "        \"Vectorized Binary\",\n",
    "        \"Multi-Class\",  # One-Versus-All\n",
    "        \"L1 Regularized\",\n",
    "        \"L2 Regularized\",\n",
    "        \"L1+L2 Regularized\",\n",
    "        \"Regularized Multi-Class\",\n",
    "        \"Stochastic\",\n",
    "        \"Steepest Gradient\",\n",
    "        \"Hessian\",\n",
    "        \"Hessian Rank Two\",\n",
    "        \"Hessian Scipy\",\n",
    "        \"Hessian BFGS+Newton Multi-Class\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_test: Optional[pd.DataFrame] = None,\n",
    "        y_test: Optional[pd.DataFrame] = None,\n",
    "    ):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.model_name = None\n",
    "        self.results = []\n",
    "    \n",
    "    def model_accuracy(self, model):\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        predictions = model.predict(self.x_test)\n",
    "        return accuracy_score(self.y_test, predictions)\n",
    "\n",
    "    def print_models_scores(self):\n",
    "        print(f\"|{''.center(81,'=')}|\")\n",
    "        print(f\"|{'MODEL'.center(40, ' ')}|{'ACCURACY'.center(40, ' ')}|\")\n",
    "        print(f\"|{''.center(81,'=')}|\")\n",
    "        for name, accuracy in self.results:\n",
    "            print(f\"|{name.center(40, ' ')}|{str(round(accuracy, 4)).center(40, ' ')}|\")\n",
    "        print(f\"|{''.center(81,'=')}|\")\n",
    "\n",
    "    def select_model(self, model_name, *args, **kwargs):\n",
    "        if model_name == \"Binary\":\n",
    "            model = self.BinaryLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Line Search\":\n",
    "            model = self.LineSearchLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Vectorized Binary\":\n",
    "            model = self.VectorBinaryLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Multi-Class\" or model_name == \"One-Versus-All\":\n",
    "            model = self.MCLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"L1 Regularized\":\n",
    "            model = self.L1RegularizedBinaryLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"L2 Regularized\":\n",
    "            model = self.L2RegularizedBinaryLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"L1+L2 Regularized\":\n",
    "            model = self.L1AndL2RegularizedBinaryLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Regularized Multi-Class\":\n",
    "            model = self.MCRegularizedLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Stochastic\":\n",
    "            model = self.StochasticLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Steepest Gradient\":\n",
    "            model = self.SteepestGradientDescentLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Hessian\":\n",
    "            model = self.HessianLogisticRegression(*args, **kwargs)\n",
    "        elif model_name == \"Hessian Rank Two\":\n",
    "            model = self.HessianLogisticRegressionRankTwo(*args, **kwargs)\n",
    "        elif model_name == \"Hessian Scipy\":\n",
    "            model = self.HessianLogisticRegressionBFGS(*args, **kwargs)\n",
    "        elif model_name == \"Hessian BFGS+Newton Multi-Class\":\n",
    "            model = self.MultiClassLogisticRegression(*args, **kwargs)\n",
    "        else:\n",
    "            print(\"Invalid model\")\n",
    "\n",
    "        accuracy = self.model_accuracy(model)\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.results.append((model_name, accuracy))\n",
    "\n",
    "        return model\n",
    "\n",
    "    class BinaryLogisticRegressionBase:\n",
    "        def __init__(self, eta, iterations=20, C: Optional[float] = None):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "\n",
    "        def __str__(self):\n",
    "            return \"Base Binary Logistic Regression Object, Not Trainable\"\n",
    "\n",
    "        # convenience, private and static:\n",
    "        @staticmethod\n",
    "        def _sigmoid(theta):\n",
    "            return 1 / (1 + np.exp(-theta))\n",
    "\n",
    "        @staticmethod\n",
    "        def _add_intercept(X):\n",
    "            return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "        def _add_bias(self, X):\n",
    "            return self._add_intercept(X)\n",
    "\n",
    "        def predict_proba(self, X, add_intercept=True):\n",
    "            # add bias term if requested\n",
    "            Xb = self._add_intercept(X) if add_intercept else X\n",
    "            return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return \"Binary Logistic Regression Object with coefficients:\\n\" + str(\n",
    "                    self.w_\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained Binary Logistic Regression Object\"\n",
    "\n",
    "        def _get_gradient(self, X, y):\n",
    "            # programming \\sum_i (yi-g(xi))xi\n",
    "            gradient = np.zeros(self.w_.shape)  # set gradient to zero\n",
    "            for (xi, yi) in zip(X, y):\n",
    "                # the actual update inside of sum\n",
    "                gradi = (yi - self.predict_proba(xi, add_intercept=False)) * xi\n",
    "                # reshape to be column vector and add to gradient\n",
    "                gradient += gradi.reshape(self.w_.shape)\n",
    "\n",
    "            return gradient / float(len(y))\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            Xb = self._add_intercept(X)  # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            # init weight vector to zeros\n",
    "            self.w_ = np.zeros((num_features, 1))\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient(Xb, y)\n",
    "                self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "\n",
    "    class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "\n",
    "        # define custom line search for problem\n",
    "        def __init__(self, iterations=0.0, C=0.001, **kwargs):\n",
    "            self.iterations = iterations\n",
    "            self.C = C\n",
    "            # but keep other keywords\n",
    "            super().__init__(**kwargs)  # call parent initializer\n",
    "\n",
    "        # this defines the function with the first input to be optimized\n",
    "        # therefore eta will be optimized, with all inputs constant\n",
    "        @staticmethod\n",
    "        def objective_function(eta, X, y, w, grad, C):\n",
    "            wnew = w - grad * eta\n",
    "            g = expit(X @ wnew)\n",
    "            # the line search is looking for minimization, so take the negative of l(w)\n",
    "            return (\n",
    "                -np.sum(np.ma.log(g[y == 1]))\n",
    "                - np.ma.sum(np.log(1 - g[y == 0]))\n",
    "                + C * sum(wnew**2)\n",
    "            )\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            Xb = self._add_bias(X)  # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = -self._get_gradient(Xb, y)\n",
    "                # minimization is in opposite direction\n",
    "\n",
    "                # do line search in gradient direction, using scipy function\n",
    "                opts = {\n",
    "                    \"maxiter\": self.iterations\n",
    "                } \n",
    "                res = scipy.optimize.minimize_scalar(\n",
    "                    self.objective_function,  # objective function to optimize\n",
    "                    bounds=(0, self.eta * 10),  # bounds to optimize\n",
    "                    args=(\n",
    "                        Xb,\n",
    "                        y,\n",
    "                        self.w_,\n",
    "                        gradient,\n",
    "                        self.C,\n",
    "                    ),  \n",
    "                    method=\"bounded\",  # bounded optimization for speed\n",
    "                    options=opts,\n",
    "                )  # set max iterations\n",
    "\n",
    "                eta = res.x  # get optimal learning rate\n",
    "                self.w_ -= gradient * eta  # set new function values\n",
    "\n",
    "    class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Vector Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained Vector Binary Logistic Regression Object\"\n",
    "\n",
    "        # inherit from our previous class to get same functionality\n",
    "        @staticmethod\n",
    "        def _sigmoid(theta):\n",
    "            # increase stability, redefine sigmoid operation\n",
    "            return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "        # but overwrite the gradient calculation\n",
    "        def _get_gradient(self, X, y):\n",
    "            ydiff = (\n",
    "                y - self.predict_proba(X, add_intercept=False).ravel()\n",
    "            )  # get y difference\n",
    "            gradient = np.mean(\n",
    "                X * ydiff[:, np.newaxis], axis=0\n",
    "            )  # make ydiff a column vector and multiply through\n",
    "\n",
    "            return gradient.reshape(self.w_.shape)\n",
    "\n",
    "    class MCLogisticRegression:\n",
    "        def __init__(self, eta, iterations=20, **kwargs):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"MultiClass Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )\n",
    "            else:\n",
    "                return \"Untrained MultiClass Logistic Regression Object\"\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            num_samples, num_features = X.shape\n",
    "            self.unique_ = np.unique(y)  # get each unique class value\n",
    "            num_unique_classes = len(self.unique_)\n",
    "            self.classifiers_ = []  # will fill this array with binary classifiers\n",
    "\n",
    "            for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "                y_binary = y == yval  # create a binary problem\n",
    "                # train the binary classifier for this class\n",
    "                blr = VectorBinaryLogisticRegression(self.eta, self.iters)\n",
    "                blr.fit(X, y_binary)\n",
    "                # add the trained classifier to the list\n",
    "                self.classifiers_.append(blr)\n",
    "\n",
    "            # save all the weights into one matrix, separate column for each class\n",
    "            self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            probs = []\n",
    "            for blr in self.classifiers_:\n",
    "                probs.append(\n",
    "                    blr.predict_proba(X)\n",
    "                )  # get probability for each classifier\n",
    "\n",
    "            return np.hstack(probs)  # make into single matrix\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.unique_[\n",
    "                np.argmax(self.predict_proba(X), axis=1)\n",
    "            ]  # take argmax along row\n",
    "\n",
    "    class L1RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "        # extend init functions\n",
    "        def __init__(self, C=0.0, **kwargs):\n",
    "            # need to add to the original initializer\n",
    "            self.C = C\n",
    "            # but keep other keywords\n",
    "            super().__init__(**kwargs)  # call parent initializer\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Regularized Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return (\n",
    "                    \"Untrained Regularized Binary Logistic Logistic Regression Object\"\n",
    "                )\n",
    "\n",
    "        # extend previous class to change functionality\n",
    "        def _get_gradient(self, X, y):\n",
    "            # call get gradient from previous class\n",
    "            gradient = super()._get_gradient(X, y)\n",
    "            gradient[1:] += -2 * np.sign(self.w_[1:]) * self.C\n",
    "            return gradient\n",
    "\n",
    "    class L2RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "        # extend init functions\n",
    "        def __init__(self, C=0.0, **kwargs):\n",
    "            # need to add to the original initializer\n",
    "            self.C = C\n",
    "            # but keep other keywords\n",
    "            super().__init__(**kwargs)  # call parent initializer\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Regularized Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return (\n",
    "                    \"Untrained Regularized Binary Logistic Logistic Regression Object\"\n",
    "                )\n",
    "\n",
    "        # extend previous class to change functionality\n",
    "        def _get_gradient(self, X, y):\n",
    "            # call get gradient from previous class\n",
    "            gradient = super()._get_gradient(X, y)\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "            return gradient\n",
    "\n",
    "    class L1AndL2RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "        # extend init functions\n",
    "        def __init__(self, C=0.0, **kwargs):\n",
    "            # need to add to the original initializer\n",
    "            self.C = C\n",
    "            # but keep other keywords\n",
    "            super().__init__(**kwargs)  # call parent initializer\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Regularized Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return (\n",
    "                    \"Untrained Regularized Binary Logistic Logistic Regression Object\"\n",
    "                )\n",
    "\n",
    "        # extend previous class to change functionality\n",
    "        def _get_gradient(self, X, y):\n",
    "            # call get gradient from previous class\n",
    "            gradient = super()._get_gradient(X, y)\n",
    "            l1gradient = -2 * np.sign(self.w_[1:]) * self.C\n",
    "            l2gradient = -2 * self.w_[1:] * self.C\n",
    "            gradient[1:] += l1gradient\n",
    "            gradient[1:] += l2gradient\n",
    "            return gradient\n",
    "\n",
    "    class MCRegularizedLogisticRegression(MCLogisticRegression):\n",
    "        def __init__(self, C=0.0, **kwargs):\n",
    "            # need to add to the original initializer\n",
    "            self.C = C\n",
    "            # but keep other keywords\n",
    "            super().__init__(**kwargs)  # call parent initializer\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Regularized Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Regularized Logistic Regression Object\"\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            num_samples, num_features = X.shape\n",
    "            self.unique_ = np.unique(y)  # get each unique class value\n",
    "            num_unique_classes = len(self.unique_)\n",
    "            self.classifiers_ = []  # will fill this array with binary classifiers\n",
    "\n",
    "            for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "                y_binary = y == yval  # create a binary problem\n",
    "                # train the binary classifier for this class\n",
    "                # now this has regularization built into it\n",
    "                blr = RegularizedBinaryLogisticRegression(\n",
    "                    eta=self.eta, iterations=self.iters, C=self.C\n",
    "                )\n",
    "                blr.fit(X, y_binary)\n",
    "                # add the trained classifier to the list\n",
    "                self.classifiers_.append(blr)\n",
    "\n",
    "            # save all the weights into one matrix, separate column for each class\n",
    "            self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    class StochasticLogisticRegression:\n",
    "        def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "            self.C = C\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Stochastic Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained Stochastic Logistic Regression Object\"\n",
    "\n",
    "        @staticmethod\n",
    "        def _add_bias(X):\n",
    "            return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid(theta):\n",
    "            # increase stability, redefine sigmoid operation\n",
    "            return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "        # vectorized gradient calculation with regularization using L2 Norm\n",
    "        def _get_gradient(self, X, y):\n",
    "            idx = np.random.randint(0, len(y))  # Grab random instance\n",
    "            # idx = int(np.random.rand() * len(y))  # grab random instance\n",
    "\n",
    "            y_val = y[idx]\n",
    "            x_val = self.predict_proba(X[idx], add_bias=False)\n",
    "            ydiff = y_val - x_val  # get y difference (now scalar)\n",
    "\n",
    "            gradient = (\n",
    "                X[idx] * ydiff[:, np.newaxis]\n",
    "            )  # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "            return gradient\n",
    "\n",
    "        def predict_proba(self, X, add_bias=True):\n",
    "            # add bias term if requested\n",
    "            Xb = self._add_bias(X) if add_bias else X\n",
    "            return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            X = X.to_numpy()\n",
    "            y = y.to_numpy()\n",
    "            Xb = self._add_bias(X)  # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for ctr, _ in enumerate(range(self.iters)):\n",
    "                gradient = self._get_gradient(Xb, y)\n",
    "                self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "\n",
    "    class SteepestGradientDescentLogisticRegression:\n",
    "        def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "            self.C = C\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Steepest Gradient Descent Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained Steepest Gradient Descent Logistic Regression Object\"\n",
    "\n",
    "        @staticmethod\n",
    "        def _add_bias(X):\n",
    "            return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid(theta):\n",
    "            # increase stability, redefine sigmoid operation\n",
    "            return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "        # vectorized gradient calculation with regularization using L2 Norm\n",
    "        def _get_gradient(self, X, y):\n",
    "            idx = np.random.randint(0, len(y))  # Grab random instance\n",
    "            # idx = int(np.random.rand() * len(y))  # grab random instance\n",
    "\n",
    "            y_val = y[idx]\n",
    "            x_val = self.predict_proba(X[idx], add_bias=False)\n",
    "            ydiff = y_val - x_val  # get y difference (now scalar)\n",
    "\n",
    "            gradient = (\n",
    "                X[idx] * ydiff[:, np.newaxis]\n",
    "            )  # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "            return gradient\n",
    "\n",
    "        def predict_proba(self, X, add_bias=True):\n",
    "            # add bias term if requested\n",
    "            Xb = self._add_bias(X) if add_bias else X\n",
    "            return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            X = X.to_numpy()\n",
    "            y = y.to_numpy()\n",
    "            Xb = self._add_bias(X)  # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            # self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "            self.w_ = np.zeros(num_features)\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for ctr, _ in enumerate(range(self.iters)):\n",
    "                # gradient = self._get_gradient(Xb, y)\n",
    "                # self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "\n",
    "                # Calculate the predicted values\n",
    "                z = np.dot(Xb, self.w_)\n",
    "                h = 1 / (1 + np.exp(-z))\n",
    "                # Calculate the error\n",
    "                error = h - y\n",
    "                # Calculate the gradient\n",
    "                gradient = np.dot(Xb.T, error) / num_samples\n",
    "                # Update the parameters\n",
    "                self.w_ = self.w_ - self.eta * gradient\n",
    "\n",
    "    class HessianLogisticRegression:\n",
    "        def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "            self.C = C\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Hessian Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained Hessian Binary Logistic Regression Object\"\n",
    "\n",
    "        @staticmethod\n",
    "        def _add_bias(X):\n",
    "            return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid(theta):\n",
    "            # increase stability, redefine sigmoid operation\n",
    "            return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "        # vectorized gradient calculation with regularization using L2 Norm\n",
    "        def _get_gradient(self, X, y):\n",
    "            g = self.predict_proba(\n",
    "                X, add_bias=False\n",
    "            ).ravel()  # get sigmoid value for all classes\n",
    "            hessian = (\n",
    "                X.T @ np.diag(g * (1 - g)) @ X - 2 * self.C\n",
    "            )  # calculate the hessian\n",
    "\n",
    "            ydiff = y - g  # get y difference\n",
    "            gradient = np.sum(\n",
    "                X * ydiff[:, np.newaxis], axis=0\n",
    "            )  # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "            return np.linalg.pinv(hessian) @ gradient\n",
    "\n",
    "        def predict_proba(self, X, add_bias=True):\n",
    "            # add bias term if requested\n",
    "            Xb = self._add_bias(X) if add_bias else X\n",
    "            return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            X = X.to_numpy()\n",
    "            y = y.to_numpy()\n",
    "            Xb = self._add_bias(X)  # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features, 1))  # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for ctr, _ in enumerate(range(self.iters)):\n",
    "                gradient = self._get_gradient(Xb, y)\n",
    "                self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "\n",
    "    class HessianLogisticRegressionRankTwo:\n",
    "        def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "            self.C = C\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Hessian Binary Logistic Regression BFGS Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained Hessian Binary Logistic Regression BFGS Object\"\n",
    "\n",
    "        @staticmethod\n",
    "        def _add_bias(X):\n",
    "            return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid(theta):\n",
    "            # increase stability, redefine sigmoid operation\n",
    "            return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "        @staticmethod\n",
    "        def objective_function(w, X, y, C):\n",
    "            g = expit(X @ w)\n",
    "            # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "            return (\n",
    "                -np.sum(np.ma.log(g[y == 1]))\n",
    "                - np.sum(np.ma.log(1 - g[y == 0]))\n",
    "                + C * sum(w**2)\n",
    "            )\n",
    "            # -np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "        @staticmethod\n",
    "        def objective_gradient(w, X, y, C):\n",
    "            g = expit(X @ w)\n",
    "            ydiff = y - g  # get y difference\n",
    "            gradient = np.mean(X * ydiff[:, np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)\n",
    "            gradient[1:] += -2 * w[1:] * C\n",
    "            return -gradient\n",
    "\n",
    "        @staticmethod\n",
    "        def backtracking_line_search(f, grad_f, x, p, *args, alpha=1, beta=0.5):\n",
    "            \"\"\"Perform a backtracking line search to find a step size that\n",
    "            satisfies the Armijo condition.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            f : callable\n",
    "                Objective function to minimize.\n",
    "            grad_f : callable\n",
    "                Gradient function of the objective function.\n",
    "            x : array-like\n",
    "                Current point in the search space.\n",
    "            p : array-like\n",
    "                Search direction.\n",
    "            args : tuple\n",
    "                Extra arguments to pass to the objective and gradient functions.\n",
    "            alpha : float, optional\n",
    "                Initial step size.\n",
    "            beta : float, optional\n",
    "                Factor by which to decrease the step size at each iteration.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float\n",
    "                Step size that satisfies the Armijo condition.\n",
    "            \"\"\"\n",
    "            while f(x + alpha * p, *args) > f(x, *args) + alpha * beta * np.dot(\n",
    "                grad_f(x, *args).T, p\n",
    "            ):\n",
    "                alpha *= beta\n",
    "            return alpha\n",
    "\n",
    "        def bfgs(\n",
    "            self, f, x0, grad_f, args=(), epsilon=1e-5, max_iterations=10, verbose=False\n",
    "        ):\n",
    "            \"\"\"Minimize a function using the BFGS algorithm.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            f : callable\n",
    "                Objective function to minimize.\n",
    "            x0 : array-like\n",
    "                Initial guess for the solution.\n",
    "            grad_f : callable\n",
    "                Gradient function of the objective function.\n",
    "            args : tuple, optional\n",
    "                Extra arguments to pass to the objective and gradient functions.\n",
    "            epsilon : float, optional\n",
    "                Tolerance for the norm of the gradient.\n",
    "            max_iterations : int, optional\n",
    "                Maximum number of iterations to perform.\n",
    "            verbose : bool, optional\n",
    "                Whether to print information about the optimization process.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            x : ndarray\n",
    "                The solution found by the optimization algorithm.\n",
    "            \"\"\"\n",
    "\n",
    "            x = x0\n",
    "            fx = f(x, *args)\n",
    "            grad_fx = grad_f(x, *args)\n",
    "            H = np.eye(x0.shape[0])\n",
    "\n",
    "            for i in range(max_iterations):\n",
    "                if np.linalg.norm(grad_fx) < epsilon:\n",
    "                    if verbose:\n",
    "                        print(f\"Optimization converged in {i} iterations.\")\n",
    "                    return x\n",
    "\n",
    "                p = -np.dot(H, grad_fx)\n",
    "                alpha = self.backtracking_line_search(f, grad_f, x, p, *args)\n",
    "                x_next = x + alpha * p\n",
    "                s = x_next - x\n",
    "                y = grad_f(x_next, *args) - grad_fx\n",
    "                rho = 1 / np.dot(y.T, s)\n",
    "                H = (np.eye(x.shape[0]) - rho * np.outer(s, y)) @ H @ (\n",
    "                    np.eye(x.shape[0]) - rho * np.outer(y, s)\n",
    "                ) + rho * np.outer(s, s)\n",
    "                x = x_next\n",
    "                fx = f(x, *args)\n",
    "                grad_fx = grad_f(x, *args)\n",
    "\n",
    "                if verbose and i % 10 == 0:\n",
    "                    print(\n",
    "                        f\"Iteration {i}: f(x)={fx}, ||grad f(x)||={np.linalg.norm(grad_fx)}\"\n",
    "                    )\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Optimization stopped after {max_iterations} iterations.\")\n",
    "            return x\n",
    "\n",
    "        def predict_proba(self, X, add_bias=True):\n",
    "            # add bias term if requested\n",
    "            Xb = self._add_bias(X) if add_bias else X\n",
    "            return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            Xb = self._add_bias(X)  # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            # Run bfgs here....\n",
    "            self.w_ = self.bfgs(\n",
    "                self.objective_function,\n",
    "                np.zeros((num_features,)),\n",
    "                self.objective_gradient,\n",
    "                args=(Xb, y, self.C),\n",
    "                epsilon=1e-3,\n",
    "                max_iterations=self.iters,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            self.w_ = self.w_.reshape((num_features, 1))\n",
    "\n",
    "    class HessianLogisticRegressionBFGS:\n",
    "        def __init__(self, eta=0.005, iterations=20, C=0.001):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "            self.C = C\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"Hessian Binary Logistic Regression BFGS Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained Hessian Binary Logistic Regression BFGS Object\"\n",
    "\n",
    "        @staticmethod\n",
    "        def _add_bias(X):\n",
    "            return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid(theta):\n",
    "            # increase stability, redefine sigmoid operation\n",
    "            return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "        @staticmethod\n",
    "        def objective_function(w, X, y, C):\n",
    "            g = expit(X @ w)\n",
    "            # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "            return (\n",
    "                -np.sum(np.ma.log(g[y == 1]))\n",
    "                - np.sum(np.ma.log(1 - g[y == 0]))\n",
    "                + C * sum(w**2)\n",
    "            )\n",
    "            # -np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "        @staticmethod\n",
    "        def objective_gradient(w, X, y, C):\n",
    "            g = expit(X @ w)\n",
    "            ydiff = y - g  # get y difference\n",
    "            gradient = np.mean(X * ydiff[:, np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)\n",
    "            gradient[1:] += -2 * w[1:] * C\n",
    "            return -gradient\n",
    "\n",
    "        def predict_proba(self, X, add_bias=True):\n",
    "            # add bias term if requested\n",
    "            Xb = self._add_bias(X) if add_bias else X\n",
    "            return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            Xb = self._add_bias(X)  # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = scipy.optimize.fmin_bfgs(\n",
    "                self.objective_function,  # what to optimize\n",
    "                np.zeros((num_features, 1)),  # starting point\n",
    "                fprime=self.objective_gradient,  # gradient function\n",
    "                args=(\n",
    "                    Xb,\n",
    "                    y,\n",
    "                    self.C,\n",
    "                ),  # extra args for gradient and objective function\n",
    "                gtol=1e-03,  # stopping criteria for gradient, |v_k|\n",
    "                maxiter=self.iters,  # stopping criteria iterations\n",
    "                disp=False,\n",
    "            )\n",
    "\n",
    "            self.w_ = self.w_.reshape((num_features, 1))\n",
    "\n",
    "    class MultiClassLogisticRegression:\n",
    "        def __init__(\n",
    "            self, eta, iterations=20, C=0.0001, solver=HessianLogisticRegressionBFGS\n",
    "        ):\n",
    "            self.eta = eta\n",
    "            self.iters = iterations\n",
    "            self.C = C\n",
    "            self.solver = solver\n",
    "            self.classifiers_ = []\n",
    "            # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "        def __str__(self):\n",
    "            if hasattr(self, \"w_\"):\n",
    "                return (\n",
    "                    \"MultiClass Logistic Regression Object with coefficients:\\n\"\n",
    "                    + str(self.w_)\n",
    "                )  # is we have trained the object\n",
    "            else:\n",
    "                return \"Untrained MultiClass Logistic Regression Object\"\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            num_samples, num_features = X.shape\n",
    "            self.unique_ = np.sort(np.unique(y))  # get each unique class value\n",
    "            num_unique_classes = len(self.unique_)\n",
    "            self.classifiers_ = []\n",
    "            for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "                y_binary = np.array(y == yval).astype(int)  # create a binary problem\n",
    "                # train the binary classifier for this class\n",
    "\n",
    "                hblr = self.solver(eta=self.eta, iterations=self.iters, C=self.C)\n",
    "                hblr.fit(X, y_binary)\n",
    "\n",
    "                # add the trained classifier to the list\n",
    "                self.classifiers_.append(hblr)\n",
    "\n",
    "            # save all the weights into one matrix, separate column for each class\n",
    "            self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            probs = []\n",
    "            for hblr in self.classifiers_:\n",
    "                probs.append(\n",
    "                    hblr.predict_proba(X).reshape((len(X), 1))\n",
    "                )  # get probability for each classifier\n",
    "\n",
    "            return np.hstack(probs)  # make into single matrix\n",
    "\n",
    "        def predict(self, X):\n",
    "            return np.argmax(self.predict_proba(X), axis=1)  # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756368c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector = ModelSelector(x_train, y_train, x_test, y_test)\n",
    "model_names = model_selector.models\n",
    "kwargs = {'eta': 0.005, 'iterations': 500, 'C': 0.001}\n",
    "for model_name in model_names:\n",
    "    model = model_selector.select_model(model_name, **kwargs)\n",
    "model_selector.print_models_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca30bf",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "We will be using this section to discuss which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0f6ab",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In summary, this section will be used to summarize the project goal and how we achieved it, and wrapping up any ideas presented back in the business understanding section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec027b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] https://console.cloud.google.com/marketplace/details/usafacts-public-data/covid19-us-cases?filter=solution-type:dataset&filter=category:covid19&id=3eaff9c5-fbaf-47bb-a441-89db1e1395ab&project=still-nebula-398202\n",
    "\n",
    "#### PLEASE COPY AND PASTE THE RESOURCES FROM THEIR CURRENT LOCATIONS AND PUT THEM HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
