{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec67695",
   "metadata": {},
   "source": [
    "# Lab Assignment Two: Exploring Image Data \n",
    "\n",
    "## CS 7324 Fall 2023\n",
    "## Catherine Magee, Morgan Mote, Luv Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18a32e",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb2de7",
   "metadata": {},
   "source": [
    "## Buisness Understanding \n",
    "\n",
    "This dataset is constructed from programs made available by NIST (National Institute of Standards and Technology). The data was collected from a total of 43 people and represents each of their handwriting. The dataset consists of 5,620 instances(rows) and 64 features (columns). The purpose of this dataset is to classify peoples handwriting of numerical values by using object recognition. There are many third parties that would benefit from object reconition such as agricuture and security. For exmaple, an agriculture company can use object recognition to detect the difference between a seedling and a dead plant, and in security we can use object recognition to alert staff of a problematic object. Further object reconition can be used for converting text into speech, which helps blind and visually impared individuals. \n",
    "    \n",
    "    \n",
    "\n",
    " #### Still need to be answered for buisness. \n",
    "Why is this data important? Once you begin modeling, how well would your prediction algorithm need to perform to be considered useful to the identified third parties? Be specific and use your own words to describe the aspects of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7c29f",
   "metadata": {},
   "source": [
    "## Loading the Dataset: Optical Recognition of Handwritten Digits\n",
    "\n",
    "### Data comes as pandas dataframe but will be converted to numpy area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8095e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 80, 'name': 'Optical Recognition of Handwritten Digits', 'repository_url': 'https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits', 'data_url': 'https://archive.ics.uci.edu/static/public/80/data.csv', 'abstract': 'Two versions of this database available; see folder', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 5620, 'num_features': 64, 'feature_types': ['Integer'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1998, 'last_updated': 'Wed Aug 23 2023', 'dataset_doi': '10.24432/C50P49', 'creators': ['E. Alpaydin', 'C. Kaynak'], 'intro_paper': {'title': 'Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition', 'authors': 'C. Kaynak', 'published_in': 'MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University', 'year': 1995, 'url': None, 'doi': None}, 'additional_info': {'summary': 'We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\\r\\n\\r\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'All input attributes are integers in the range 0..16.\\r\\nThe last attribute is the class code 0..9', 'citation': None}}\n",
      "           name     role         type demographic description units  \\\n",
      "0    Attribute1  Feature      Integer        None        None  None   \n",
      "1    Attribute2  Feature      Integer        None        None  None   \n",
      "2    Attribute3  Feature      Integer        None        None  None   \n",
      "3    Attribute4  Feature      Integer        None        None  None   \n",
      "4    Attribute5  Feature      Integer        None        None  None   \n",
      "..          ...      ...          ...         ...         ...   ...   \n",
      "60  Attribute61  Feature      Integer        None        None  None   \n",
      "61  Attribute62  Feature      Integer        None        None  None   \n",
      "62  Attribute63  Feature      Integer        None        None  None   \n",
      "63  Attribute64  Feature      Integer        None        None  None   \n",
      "64        class   Target  Categorical        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "..            ...  \n",
      "60             no  \n",
      "61             no  \n",
      "62             no  \n",
      "63             no  \n",
      "64             no  \n",
      "\n",
      "[65 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# pip install ucimlrepo\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "optical_recognition_of_handwritten_digits = fetch_ucirepo(id=80) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = optical_recognition_of_handwritten_digits.data.features \n",
    "y = optical_recognition_of_handwritten_digits.data.targets\n",
    "\n",
    "\n",
    "\n",
    "# metadata \n",
    "print(optical_recognition_of_handwritten_digits.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(optical_recognition_of_handwritten_digits.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8bd4369",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#_, h, w = optical_recognition_of_handwritten_digits.images.shape\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumbers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misfinite(X)))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Since our data set was read in as a pandas dataframe above we will convert them to numpy arrays. \n",
    "# If running for the first time continue. If second time or rerun comment out the X to numpy and y to numpy.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "numbers = optical_recognition_of_handwritten_digits.target_col\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "#_, h, w = optical_recognition_of_handwritten_digits.images.shape\n",
    "n_classes = len(numbers)\n",
    "\n",
    "\n",
    "print(np.sum(~np.isfinite(X)))\n",
    "print(\"n_samples: {}\".format(n_samples))\n",
    "print(\"n_features: {}\".format(n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d1f69",
   "metadata": {},
   "source": [
    "## Data Preparation (1 points total)\n",
    "\n",
    "[.5 points] Read in your images as numpy arrays. Resize and recolor images as necessary. \n",
    "[.4 points] Linearize the images to create a table of 1-D image features (each row should be one image).   \n",
    "[.1 points] Visualize several images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a2aa4",
   "metadata": {},
   "source": [
    "## Data Reduction (6 points total)\n",
    "\n",
    "[.5 points] Perform linear dimensionality reduction of the images using principal components analysis. Visualize the explained variance of each component. Analyze how many dimensions are required to adequately represent your image data. Explain your analysis and conclusion.\n",
    "[.5 points] Perform linear dimensionality reduction of your image data using randomized principle components analysis. Visualize the explained variance of each component. Analyze how many dimensions are required to adequately represent your image data. Explain your analysis and conclusion.\n",
    "[2 points]  Compare the representation using PCA and Randomized PCA. The method you choose to compare dimensionality methods should quantitatively explain which method is better at representing the images with fewer components.  Do you prefer one method over another? Why?\n",
    "[1 points] Perform feature extraction upon the images using any feature extraction technique (e.g., gabor filters, ordered gradients, DAISY, etc.).\n",
    "[2 points] Does this feature extraction method show promise for your prediction task? Why? Use visualizations to analyze this questions. For example, visualize the differences between statistics of extracted features in each target class. Another option, use a heat map of the pairwise differences (ordered by class) among all extracted features. Another option, build a nearest neighbor classifier to see actual classification performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340592f",
   "metadata": {},
   "source": [
    " ## Exceptional Work (1 points total)\n",
    " \n",
    " You have free reign to provide any additional analyses. \n",
    "One idea (required for 7000 level students): Perform feature extraction upon the images using DAISY. Rather than using matching on the images with the total DAISY vector, you will instead use key point matching. You will need to investigate appropriate methods for key point matching using DAISY. NOTE: this often requires some type of brute force matching per pair of images, which can be computationally expensive. Does it do better with key point versus not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b781e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
