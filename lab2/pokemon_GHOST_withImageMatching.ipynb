{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa567f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path \n",
    "from keras.utils import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecfce1",
   "metadata": {},
   "source": [
    "# Lab Assignment Two: Exploring Image Data \n",
    "\n",
    "## CS 7324 Fall 2023\n",
    "## Catherine Magee, Morgan Mote, Luv Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03042d",
   "metadata": {},
   "source": [
    "## Buisness Understanding \n",
    "\n",
    "\n",
    "This exploratory analysis focuses on a dataset of the Generation one Pokemon! We pulled this dataset from kaggle. This datset is 2gb consisting of 151 folders. Each folder is labeled as a generation one pokemon and contains 60 images. Therefore our dataset looks at 151 generation one pokemon and 60 images of each Pokemon! Which means there are over 9,000 images in our dataset. This dataset was originally built to preserve the first generation Pokemon's names and photos to prevent data sanitization.\n",
    "\n",
    "\n",
    "#### To be completed: \n",
    "What is the prediction task for your dataset and which third parties would be interested in the results? Why is this data important? Once you begin modeling, how well would your prediction algorithm need to perform to be considered useful to the identified third parties? Be specific and use your own words to describe the aspects of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29346d",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "\n",
    "## Loading the Dataset: Pokemon Generation One\n",
    "https://www.kaggle.com/datasets/thedagger/pokemon-generation-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53459da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(r\"C:\\Users\\morga\\OneDrive\\Desktop\\lab_2\\dataset\")\n",
    "\n",
    "# Regular expression that pulls all the directories (folders in that path)\n",
    "dirs = p.glob(\"*\")\n",
    "\n",
    "# prints all the pokemon data folders we will testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031b54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries used below in the data visualization and linearization\n",
    "\n",
    "labels_dict = {'Zapdos': 0, 'Kadabra': 1, 'Omanyte': 2, 'Shellder': 3, 'Bellsprout': 4, 'Eevee': 5, 'Jolteon': 6, \n",
    "               'Hypno': 7, 'Seel': 8, 'Zubat': 9, 'Graveler': 10, 'Magneton': 11, 'Abra': 12, 'Kingler': 13, \n",
    "               'Alakazam': 14, 'Clefable': 15, 'Gyarados': 16, '.DS_Store': 17, 'Poliwag': 18, 'Rapidash': 19, \n",
    "               'Machamp': 20, 'Pinsir': 21, 'Muk': 22, 'Seaking': 23, 'Magikarp': 24, 'Goldeen': 25, 'Venusaur': 26, \n",
    "               'Flareon': 27, 'Jigglypuff': 28, 'Doduo': 29, 'Weedle': 30, 'Vileplume': 31, 'Arcanine': 32, \n",
    "               'Tentacruel': 33, 'Gloom': 34, 'Charmeleon': 35, 'Articuno': 36, 'Sandshrew': 37, 'Spearow': 38, \n",
    "               'Marowak': 39, 'Clefairy': 40, 'dataset': 41, 'Snorlax': 42, 'Scyther': 43, 'Primeape': 44, \n",
    "               'Diglett': 45, 'Onix': 46, 'Mankey': 47, 'Rattata': 48, 'Voltorb': 49, 'Gengar': 50, 'Gastly': 51, \n",
    "               'Cloyster': 52, 'Weepinbell': 53, 'Dragonair': 54, 'Squirtle': 55, 'Pikachu': 56, 'Victreebel': 57, \n",
    "               'Charmander': 58, 'Staryu': 59, 'Venonat': 60, 'Vaporeon': 61, 'Ivysaur': 62, 'Krabby': 63, \n",
    "               'Drowzee': 64, 'Sandslash': 65, 'Kangaskhan': 66, 'Chansey': 67, 'Butterfree': 68, 'Starmie': 69,\n",
    "               'Magmar': 70, 'Beedrill': 71, 'Ninetales': 72, 'Magnemite': 73, 'Metapod': 74, 'Electrode': 75, \n",
    "               'Raichu': 76, 'Fearow': 77, 'Mewtwo': 78, 'Kabuto': 79, 'Pidgeotto': 80, 'Hitmonchan': 81, \n",
    "               'Blastoise': 82, 'Weezing': 83, 'Golbat': 84, 'Seadra': 85, 'Rhyhorn': 86, 'Moltres': 87, \n",
    "               'Golduck': 88, 'Kabutops': 89, 'Aerodactyl': 90, 'Haunter': 91, 'Machop': 92, 'Koffing': 93, \n",
    "               'Pidgeot': 94, 'Wigglytuff': 95, 'Porygon': 96, 'Vulpix': 97, 'Dugtrio': 98, 'Ditto': 99, \n",
    "               'Raticate': 100, 'Geodude': 101, 'Tentacool': 102, 'Horsea': 103, 'Oddish': 104, 'Machoke': 105, \n",
    "               'Lapras': 106, 'Poliwrath': 107, 'Omastar': 108, 'Slowpoke': 109, 'Bulbasaur': 110, 'Growlithe': 111, \n",
    "               'Ponyta': 112, 'Parasect': 113, 'Dodrio': 114, 'Meowth': 115, 'Exeggutor': 116, 'Persian': 117, \n",
    "               'Psyduck': 118, 'Tauros': 119, 'Pidgey': 120, 'Electabuzz': 121, 'Dewgong': 122, 'Wartortle': 123, \n",
    "               'Nidoking': 124, 'Grimer': 125, 'Ekans': 126, 'Caterpie': 127, 'Tangela': 128, 'Kakuna': 129, \n",
    "               'Golem': 130, 'Slowbro': 131, 'MrMime': 132, 'Jynx': 133, 'Mew': 134, 'Paras': 135, 'Hitmonlee': 136,\n",
    "               'Exeggcute': 137, 'Arbok': 138, 'Venomoth': 139, 'Dratini': 140, 'Cubone': 141, 'Rhydon': 142,\n",
    "               'Dragonite': 143, 'Nidorino': 144, 'Lickitung': 145, 'Nidorina': 146, 'Charizard': 147, \n",
    "               'Poliwhirl': 148, 'Nidoqueen': 149, 'Farfetchd': 150}\n",
    "\n",
    "dict_label = {0: 'Zapdos', 1: 'Kadabra', 2: 'Omanyte', 3: 'Shellder', 4: 'Bellsprout', 5: 'Eevee', 6: 'Jolteon', \n",
    "               7: 'Hypno', 8: 'Seel', 9: 'Zubat', 10: 'Graveler', 11: 'Magneton', 12: 'Abra', 13: 'Kingler', 14: \n",
    "               'Alakazam', 15: 'Clefable', 16: 'Gyarados', 17: '.DS_Store', 18: 'Poliwag', 19: 'Rapidash', \n",
    "               20: 'Machamp', 21: 'Pinsir', 22: 'Muk', 23: 'Seaking', 24: 'Magikarp', 25: 'Goldeen', 26: 'Venusaur', \n",
    "               27: 'Flareon', 28: 'Jigglypuff', 29: 'Doduo', 30: 'Weedle', 31: 'Vileplume', 32: 'Arcanine', \n",
    "               33: 'Tentacruel', 34: 'Gloom', 35: 'Charmeleon', 36: 'Articuno', 37: 'Sandshrew', 38: 'Spearow', \n",
    "               39: 'Marowak', 40: 'Clefairy', 41: 'dataset', 42: 'Snorlax', 43: 'Scyther', 44: 'Primeape', \n",
    "               45: 'Diglett', 46: 'Onix', 47: 'Mankey', 48: 'Rattata', 49: 'Voltorb', 50: 'Gengar', \n",
    "               51: 'Gastly', 52: 'Cloyster', 53: 'Weepinbell', 54: 'Dragonair', 55: 'Squirtle', \n",
    "               56: 'Pikachu', 57: 'Victreebel', 58: 'Charmander', 59: 'Staryu', 60: 'Venonat', \n",
    "               61: 'Vaporeon', 62: 'Ivysaur', 63: 'Krabby', 64: 'Drowzee', 65: 'Sandslash', \n",
    "               66: 'Kangaskhan', 67: 'Chansey', 68: 'Butterfree', 69: 'Starmie', 70: 'Magmar', \n",
    "               71: 'Beedrill', 72: 'Ninetales', 73: 'Magnemite', 74: 'Metapod', 75: 'Electrode', \n",
    "               76: 'Raichu', 77: 'Fearow', 78: 'Mewtwo', 79: 'Kabuto', 80: 'Pidgeotto', 81: 'Hitmonchan', \n",
    "               82: 'Blastoise', 83: 'Weezing', 84: 'Golbat', 85: 'Seadra', 86: 'Rhyhorn', 87: 'Moltres', \n",
    "               88: 'Golduck', 89: 'Kabutops', 90: 'Aerodactyl', 91: 'Haunter', 92: 'Machop', 93: 'Koffing', \n",
    "               94: 'Pidgeot', 95: 'Wigglytuff', 96: 'Porygon', 97: 'Vulpix', 98: 'Dugtrio', 99: 'Ditto', \n",
    "               100: 'Raticate', 101: 'Geodude', 102: 'Tentacool', 103: 'Horsea', 104: 'Oddish', 105: 'Machoke', \n",
    "               106: 'Lapras', 107: 'Poliwrath', 108: 'Omastar', 109: 'Slowpoke', 110: 'Bulbasaur', 111: 'Growlithe', \n",
    "               112: 'Ponyta', 113: 'Parasect', 114: 'Dodrio', 115: 'Meowth', 116: 'Exeggutor', 117: 'Persian', \n",
    "               118: 'Psyduck', 119: 'Tauros', 120: 'Pidgey', 121: 'Electabuzz', 122: 'Dewgong', 123: 'Wartortle',\n",
    "               124: 'Nidoking', 125: 'Grimer', 126: 'Ekans', 127: 'Caterpie', 128: 'Tangela', 129: 'Kakuna',\n",
    "               130: 'Golem', 131: 'Slowbro', 132: 'MrMime', 133: 'Jynx', 134: 'Mew', 135: 'Paras', 136: 'Hitmonlee', \n",
    "               137: 'Exeggcute', 138: 'Arbok', 139: 'Venomoth', 140: 'Dratini', 141: 'Cubone', 142: 'Rhydon', \n",
    "               143: 'Dragonite', 144: 'Nidorino', 145: 'Lickitung', 146: 'Nidorina', 147: 'Charizard', \n",
    "               148: 'Poliwhirl', 149: 'Nidoqueen', 150: 'Farfetchd'}\n",
    "\n",
    "\n",
    "# Used the below to make the dictionaries I was just too lazy to type all the key value pairs \n",
    "#count = 0 \n",
    "\n",
    "#for folder_dir in dirs:\n",
    "   \n",
    "    #temp = str(folder_dir)\n",
    "    #temp = temp.split(\"/\")\n",
    "    #label = (temp[-1])\n",
    "    \n",
    "    #dict_label[count] = label\n",
    "    #count = count +1\n",
    "\n",
    "#print(dict_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581c71d",
   "metadata": {},
   "source": [
    "## Linearize the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9397f63e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_56508\\3384924740.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# loads each image in the 151 folders and flattens the pixels of each photo to a row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolder_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mimage_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mimage_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_img' is not defined"
     ]
    }
   ],
   "source": [
    "image_data = []\n",
    "labels = []\n",
    "\n",
    "# Iterates through directory to find each 151 folders; and create labels\n",
    "for folder_dir in dirs: \n",
    "    temp = str(folder_dir)\n",
    "    temp = temp.split(\"/\")\n",
    "    label = (temp[-1])\n",
    "    \n",
    "    cnt = 0 \n",
    "    \n",
    "    # loads each image in the 151 folders and flattens the pixels of each photo to a row\n",
    "    for image_path in folder_dir.glob(\"*.jpg\"):\n",
    "        image = load_img(image_path,target_size=(100,100))\n",
    "        image_arr = img_to_array(image)\n",
    "        image_data.append(image_arr)\n",
    "        labels.append(labels_dict[label])\n",
    "        \n",
    "        \n",
    "        cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aedc17",
   "metadata": {},
   "source": [
    "## Calculation on our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1528b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the num of images\n",
    "print(len(image_data))\n",
    "\n",
    "# The different pokemon have different labels\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6698bb52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_56508\\1069882544.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 10118: images, height, weight: 100, channels: 3 (because it is a colored photo (red, green, blue))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mimage_amt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "X = np.array(image_data)\n",
    "y = np.array(labels)\n",
    "\n",
    "# 10118: images, height, weight: 100, channels: 3 (because it is a colored photo (red, green, blue))\n",
    "image_amt, height, weight, channels = X.shape\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"number of samples: \" + str(image_amt))\n",
    "print(\"height of images: \" + str(height))\n",
    "print(\"weight of images: \" + str(weight))\n",
    "print(\"number channels (RBG)\" + str(channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7077b",
   "metadata": {},
   "source": [
    "## Visualize the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "733d7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw pokemon helper function! \n",
    "def drawImg(image_data, label):\n",
    "    plt.title(dict_label[label])\n",
    "    plt.imshow(image_data)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fc22f26",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 197 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46140\\538241919.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mrand_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdrawImg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 197 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# draws 10 random pokemon with their labels\n",
    "for i in range(10):\n",
    "    rand_idx = np.random.randint(400)\n",
    "    drawImg(X[rand_idx]/255, y[rand_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe6783",
   "metadata": {},
   "source": [
    "## Data Reduction (6 points total)\n",
    "\n",
    "[.5 points] Perform linear dimensionality reduction of the images using principal components analysis. Visualize the explained variance of each component. Analyze how many dimensions are required to adequately represent your image data. Explain your analysis and conclusion.\n",
    "[.5 points] Perform linear dimensionality reduction of your image data using randomized principle components analysis. Visualize the explained variance of each component. Analyze how many dimensions are required to adequately represent your image data. Explain your analysis and conclusion.\n",
    "[2 points]  Compare the representation using PCA and Randomized PCA. The method you choose to compare dimensionality methods should quantitatively explain which method is better at representing the images with fewer components.  Do you prefer one method over another? Why?\n",
    "[1 points] Perform feature extraction upon the images using any feature extraction technique (e.g., gabor filters, ordered gradients, DAISY, etc.).\n",
    "[2 points] Does this feature extraction method show promise for your prediction task? Why? Use visualizations to analyze this questions. For example, visualize the differences between statistics of extracted features in each target class. Another option, use a heat map of the pairwise differences (ordered by class) among all extracted features. Another option, build a nearest neighbor classifier to see actual classification performance. \n",
    "\n",
    " ## Exceptional Work (1 points total)\n",
    " \n",
    " You have free reign to provide any additional analyses. \n",
    "One idea (required for 7000 level students): Perform feature extraction upon the images using DAISY. Rather than using matching on the images with the total DAISY vector, you will instead use key point matching. You will need to investigate appropriate methods for key point matching using DAISY. NOTE: this often requires some type of brute force matching per pair of images, which can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define several functions to be used for image display in RGB/grayscale, PCA, calculation and display of eigen-images, and more\n",
    "# Helper Functions for Eigenvectors\n",
    "\n",
    "def load_image_rgb(image_path):\n",
    "    \"\"\"Helper function for dataframe apply to load rgb images into dataframe.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (100, 100))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    r, g, b = cv2.split(image)\n",
    "    normalize = lambda c: c / 255.0\n",
    "    r = normalize(r)\n",
    "    g = normalize(g)\n",
    "    b = normalize(b)\n",
    "    return r, g, b\n",
    "\n",
    "\n",
    "def load_image_grayscale(image_path):\n",
    "    \"\"\"Helper function for dataframe apply to load grayscale images into dataframe.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (100, 100))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return image\n",
    "\n",
    "def slice_tuple(col, idx):\n",
    "    \"\"\"Example df['col'].apply(slice_tuple, idx=0)\"\"\"\n",
    "    return col[idx]\n",
    "\n",
    "def stack_cols(col):\n",
    "    return col.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_color_channels(image_path):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    blues, greens, reds = cv2.split(image)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "\n",
    "    fig.add_subplot(2, 2, 1)\n",
    "    plt.title(\"Blue Channel\")\n",
    "    plt.imshow(blues)\n",
    "\n",
    "    fig.add_subplot(2, 2, 2)\n",
    "    plt.title(\"Green Channel\")\n",
    "    plt.imshow(greens)\n",
    "\n",
    "    fig.add_subplot(2, 2, 3)\n",
    "    plt.title(\"Red Channel\")\n",
    "    plt.imshow(reds)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b738e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_image_pca(image_path, n_components=50, show_comparison=False, **kwargs):\n",
    "    \"\"\"Reduce image to RGB using PCA\"\"\"\n",
    "    # additional_params = kwargs\n",
    "    # print(additional_params)\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    blues, greens, reds = cv2.split(image)\n",
    "\n",
    "    normalize = lambda c: c / 255.0  # DEPRECATE FOR BELOW\n",
    "\n",
    "    reds = normalize(reds)\n",
    "    blues = normalize(blues)\n",
    "    greens = normalize(greens)\n",
    "\n",
    "    pca_b = PCA(n_components, **kwargs)\n",
    "    pca_b.fit(blues)\n",
    "    xfrm_pca_b = pca_b.transform(blues)\n",
    "\n",
    "    pca_g = PCA(n_components, **kwargs)\n",
    "    pca_g.fit(greens)\n",
    "    xfrm_pca_g = pca_g.transform(greens)\n",
    "\n",
    "    pca_r = PCA(n_components, **kwargs)\n",
    "    pca_r.fit(reds)\n",
    "    xfrm_pca_r = pca_r.transform(reds)\n",
    "\n",
    "    inv_xfrm_b = pca_b.inverse_transform(xfrm_pca_b)\n",
    "    inv_xfrm_g = pca_g.inverse_transform(xfrm_pca_g)\n",
    "    inv_xfrm_r = pca_r.inverse_transform(xfrm_pca_r)\n",
    "\n",
    "    image_reduced = cv2.merge((inv_xfrm_b, inv_xfrm_g, inv_xfrm_r))\n",
    "\n",
    "    return image_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_image_pca_grayscale(image_path, n_components=50, show_comparison=False, **kwargs):\n",
    "    \"\"\"Reduce image to grayscale using PCA\"\"\"\n",
    "    # additional_params = kwargs\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.normalize(\n",
    "        image, np.zeros((image.shape[:2])), 0, 255, cv2.NORM_MINMAX\n",
    "    )\n",
    "\n",
    "    pca = PCA(n_components, **kwargs)\n",
    "    pca.fit(image)\n",
    "    xfrm_pca = pca.transform(image)\n",
    "\n",
    "    inv_xfrm = pca.inverse_transform(xfrm_pca)\n",
    "\n",
    "    image_reduced = inv_xfrm\n",
    "\n",
    "    return image_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15939cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_rgb_hist(image_path, n_components=50, show_comparison=False):\n",
    "    \"\"\"Returns blues, greens, reds data for histogram plotting\"\"\"\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    blues, greens, reds = cv2.split(image)\n",
    "\n",
    "    normalize = lambda c: c / 255.0\n",
    "    reds = normalize(reds)\n",
    "    blues = normalize(blues)\n",
    "    greens = normalize(greens)\n",
    "\n",
    "    return blues, greens, reds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ea145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_explained_variance(image_path):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(image)\n",
    "\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "    # How many PCs explain 95% of the variance?\n",
    "    k = np.argmax(cumulative_variance > 95)\n",
    "    print(\"Number of components explaining 95% variance: \" + str(k))\n",
    "\n",
    "    plt.figure(figsize=[10, 5])\n",
    "    plt.title(\"Cumulative Explained Variance explained by the components\")\n",
    "    plt.ylabel(\"Cumulative Explained variance\")\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.axvline(x=k, color=\"k\", linestyle=\"--\")\n",
    "    plt.axhline(y=95, color=\"r\", linestyle=\"--\")\n",
    "    ax = plt.plot(cumulative_variance)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reduced_image_comparison(image_path, **kwargs):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    image_rgb = reduce_image_pca(image_path, **kwargs)\n",
    "    image_grayscale = reduce_image_pca_grayscale(image_path, **kwargs)\n",
    "    blues, reds, greens = image_rgb_hist(image_path)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "\n",
    "    fig.add_subplot(2, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "    fig.add_subplot(2, 2, 2)\n",
    "    plt.title(f\"Reduced Image\")\n",
    "    plt.imshow(image_rgb)\n",
    "\n",
    "    fig.add_subplot(2, 2, 3)\n",
    "    plt.title(\"Grayscale Image\")\n",
    "    plt.imshow(image_grayscale, cmap=\"gray\")\n",
    "\n",
    "    fig.add_subplot(2, 2, 4)\n",
    "    plt.title(\"RGB Histogram\")\n",
    "    plt.hist(reds.ravel(), bins=256, color=\"red\", stacked=True)\n",
    "    plt.hist(greens.ravel(), bins=256, color=\"green\")\n",
    "    plt.hist(blues.ravel(), bins=256, color=\"blue\")\n",
    "\n",
    "    if \"n_components\" in kwargs.keys():\n",
    "        plt.suptitle(\n",
    "            f\"PCA Image Construction (n_comp={kwargs['n_components']})\"\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_incremental_components(image_path, n_components):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "    def plot_at_pos_k(image, n_components):\n",
    "        pca_incremental = IncrementalPCA(n_components)\n",
    "        image = pca_incremental.inverse_transform(\n",
    "            pca_incremental.fit_transform(image)\n",
    "        )\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    ks = [5, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "    plt.figure(figsize=[10, 9])\n",
    "\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        if i < len(ks):\n",
    "            plot_at_pos_k(image, ks[i])\n",
    "            plt.title(\"Components: \" + str(ks[i]))\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.18, hspace=0.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f07146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eigen_values(label: str, n_items: int = 100, n_components=50, **kwargs):\n",
    "    \"\"\"Example plot_eigen_values(\"pikachu\", 30, 50)\n",
    "\n",
    "    Collects 30 rows with the label dolphin and displays the\n",
    "    eigen-pictures.\n",
    "\n",
    "    n_items must be greater than or equal to n_components.\n",
    "    \"\"\"\n",
    "\n",
    "    tmp_df = df[df[\"labels\"] == label].iloc[:n_items]\n",
    "    tmp_df[\"grayscale\"] = tmp_df[\"images\"].apply(load_image_grayscale)\n",
    "    image = np.stack(tmp_df[\"grayscale\"].apply(stack_cols))\n",
    "    pca = PCA(n_components=n_components, **kwargs)\n",
    "    pca.fit(image)\n",
    "\n",
    "    # Assumes image dimensions are 100 x 100 pixels\n",
    "    eigen_values = pca.components_.reshape((n_components, 100, 100))\n",
    "\n",
    "    # Create list of titles for plot\n",
    "    eigen_titles = [f\"eigen-{label} {i}\" for i in range(eigen_values.shape[0])]\n",
    "\n",
    "    def show_gallery(images, titles, h, w, n_row=3, n_col=6):\n",
    "        \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "        plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "        plt.subplots_adjust(\n",
    "            bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35\n",
    "        )\n",
    "        for i in range(n_row * n_col):\n",
    "            plt.subplot(n_row, n_col, i + 1)\n",
    "            plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "            plt.title(titles[i], size=12)\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "\n",
    "    # Assumes image dimensions are 100 x 100 pixels\n",
    "    show_gallery(eigen_values, eigen_titles, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5793ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_image_variance_and_component_information(image_path: str):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.normalize(\n",
    "        image, np.zeros((image.shape[:2])), 0, 255, cv2.NORM_MINMAX\n",
    "    )\n",
    "    pca = PCA()\n",
    "    pca.fit(image)\n",
    "\n",
    "    msg = {\n",
    "        \"Principal components\": pca.components_[0][:10],\n",
    "        \"Number of features\": pca.n_features_in_,\n",
    "        \"Covariance\": pca.get_covariance()[0][:10],\n",
    "        \"Explained Variance\": pca.explained_variance_[:10],\n",
    "    }\n",
    "    msg = [f\"{str(k)} - {str(v)} ...\" for k, v in msg.items()]\n",
    "    for line in msg:\n",
    "        print()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_information_retention(image_path: str):\n",
    "    \"\"\"A bar plot of information retention based on components\n",
    "    of eigenvectors\n",
    "    \"\"\"\n",
    "\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.normalize(\n",
    "        image, np.zeros((image.shape[:2])), 0, 255, cv2.NORM_MINMAX\n",
    "    )\n",
    "    # pca = PCA()\n",
    "\n",
    "    covariance = np.cov(image.T)\n",
    "    eigval, eigvec = np.linalg.eig(covariance)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(\n",
    "        [1, 2, 3, 4, 5, 6, 7],\n",
    "        eigval[:7] / sum(eigval[:7]) * 100,\n",
    "        align=\"center\",\n",
    "        width=0.4,\n",
    "        tick_label=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\"],\n",
    "    )\n",
    "    plt.ylabel(\"Variance (%)\")\n",
    "    plt.title(\"Information retention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15840d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairwise_principal_components(image_path: str):\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.normalize(\n",
    "        image, np.zeros((image.shape[:2])), 0, 255, cv2.NORM_MINMAX\n",
    "    )\n",
    "    pca = PCA()\n",
    "    pca.fit(image)\n",
    "\n",
    "    sns.pairplot(\n",
    "        pd.DataFrame(\n",
    "            pca.components_[:, -5:],\n",
    "            columns=[\"PC 1\", \"PC 2\", \"PC 3\", \"PC 4\", \"PC 5\"],\n",
    "        ),\n",
    "        height=2,\n",
    "        diag_kind=\"kde\",\n",
    "        plot_kws=dict(s=8),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_using_multiple_filters(image_path: str):\n",
    "    # Base Image\n",
    "    image = cv2.imread(image_path)\n",
    "    # RGB Image\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Grayscale Image\n",
    "    image_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Median filter image\n",
    "    med_val = np.median(image)\n",
    "    lower = int(max(0, 0.7 * med_val))\n",
    "    upper = int(min(255, 1.3 * med_val))\n",
    "    blurred_img = cv2.blur(image, ksize=(5, 5))\n",
    "    image_median_filter = cv2.Canny(\n",
    "        image=blurred_img, threshold1=lower, threshold2=upper + 50\n",
    "    )\n",
    "    # Threshold filter image\n",
    "    image_threshold_filter = cv2.Canny(image, threshold1=100, threshold2=255)\n",
    "    # Binary filter image\n",
    "    max_threshold = skimage.filters.threshold_otsu(image_grayscale)\n",
    "    max_threshold = skimage.filters.threshold_otsu(image_grayscale)\n",
    "    image_binary_filter = image_grayscale > max_threshold\n",
    "    # Sobel filter image\n",
    "    v = skimage.filters.sobel_v(image_grayscale)\n",
    "    h = skimage.filters.sobel_h(image_grayscale)\n",
    "    image_sobel_filter = np.sqrt(pow(v, 2) + pow(h, 2))\n",
    "\n",
    "\n",
    "    # Setup a 3 row, 2 column plot\n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(\n",
    "        3, 2, figsize=(9, 9)\n",
    "    )\n",
    "\n",
    "    ax1.imshow(image_rgb)\n",
    "    ax1.set_title(\"original\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(image_grayscale, cmap=\"gray\")\n",
    "    ax2.set_title(\"Gray scale\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    ax3.imshow(image_binary_filter, cmap=\"gray\")\n",
    "    ax3.set_title(\"Binary Image\")\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "    ax4.imshow(image_median_filter)\n",
    "    ax4.set_title(\"Median Filter Image\")\n",
    "    ax4.axis(\"off\")\n",
    "\n",
    "    ax5.imshow(image_threshold_filter)\n",
    "    ax5.set_title(\"Threshold Filter Image\")\n",
    "    ax5.axis(\"off\")\n",
    "\n",
    "    ax6.imshow(image_sobel_filter, cmap=\"gray\")\n",
    "    ax6.set_title(\"Sobel Filter Image\")\n",
    "    ax6.axis(\"off\")\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.subplots_adjust(bottom=0, hspace=0.35)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ef701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daisy(label: str, n_rows: int = 30, idx: int = 1):\n",
    "    \"\"\"Example plot_daisy('pokemon') plots the nearest image to the index\n",
    "    image entered as the argument.\n",
    "\n",
    "    n_rows is the number of rows for the specified label.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply_daisy(row, shape):\n",
    "        feat = skimage.feature.daisy(\n",
    "            row.reshape(shape),\n",
    "            step=20,\n",
    "            radius=20,\n",
    "            rings=2,\n",
    "            histograms=8,\n",
    "            orientations=4,\n",
    "            visualize=False,\n",
    "        )\n",
    "        return feat.reshape((-1))\n",
    "    \n",
    "    tmp_df = df[df[\"labels\"] == label].iloc[:n_rows]\n",
    "    tmp_df[\"grayscale\"] = tmp_df[\"images\"].apply(load_image_grayscale)\n",
    "    image = np.stack(tmp_df[\"grayscale\"].apply(stack_cols))\n",
    "\n",
    "    features = np.apply_along_axis(apply_daisy, 1, image, (100, 100))\n",
    "    # Uses sklearn.metrics.pairwise\n",
    "    distance_matrix = pairwise.pairwise_distances(features)\n",
    "\n",
    "    distances = distance_matrix[idx, :]\n",
    "    distances[idx] = np.infty  # dont pick the same image!\n",
    "    nearest_idx = np.argmin(distances)\n",
    "\n",
    "    plt.figure(figsize=(7, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image[idx].reshape((100, 100)), cmap=\"gray\")\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image[nearest_idx].reshape((100, 100)), cmap=\"gray\")\n",
    "    plt.title(\"Closest Image\")\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select an image for individual analysis\n",
    "pokemon_image = df[df[\"labels\"] == \"pokemon\"][\"images\"].iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use principle component analysis to calculate and visualize explained variance\n",
    "image = cv2.cvtColor(cv2.imread(pokemon_image), cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d32d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image_variance_and_component_information(pokemon_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_information_retention(pokemon_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_principal_components(pokemon_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97023541",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_explained_variance(pokemon_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot eigenvalues, generate the eigen-pokemon, and visualize the image with varying numbers of principle components.\n",
    "plot_eigen_values(\"pokemon\", n_items=100, n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_incremental_components(pokemon_image, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ade06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the pokemon image down to the three color channels: blue, green, and red\n",
    "plot_image_color_channels(pokemon_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct Reduced Image (Grayscale and Color) Using PCA\n",
    "plot_reduced_image_comparison(pokemon_image, n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be938de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot eigenvalues and visualize the image - but this time by using randomized PCA.\n",
    "plot_eigen_values(\"pokemon\", svd_solver=\"randomized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec99a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_image_comparison(pokemon_image, svd_solver=\"randomized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the pokemon image using multiple filters and also visualize the DAISY descriptors.\n",
    "# find the closest image by determining the image with the least distance difference in it's \n",
    "# feature descriptors from the original (while not picking the same image)\n",
    "plot_image_using_multiple_filters(pokemon_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d11e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, img_desc = skimage.feature.daisy(\n",
    "    cv2.cvtColor(cv2.imread(pokemon_image), cv2.COLOR_BGR2GRAY),\n",
    "    step=20,\n",
    "    radius=20,\n",
    "    rings=2,\n",
    "    histograms=8,\n",
    "    orientations=8,\n",
    "    visualize=True,\n",
    ")\n",
    "plt.imshow(img_desc)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daisy('pokemon', n_rows=100, idx=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d439fb5",
   "metadata": {},
   "source": [
    "# ^^^ everywhere pokemon_image is we need to replace it with an actual image filename, and also there may be some cleaning up of the code since I havent been able to make any of this actually run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99401c8",
   "metadata": {},
   "source": [
    "### Gabor image processing to identifiy specific frequency contents with a specific direction in an area of analysis\n",
    "\n",
    "Functions used in Gabor:\n",
    "sigma σ – Standard Deviation – Property of the bell curve. Smaller values emphasize values nearer to the center\n",
    "theta θ – Direction – Identifies direction of the sine wave\n",
    "lambda λ – Wavelength – Distance between peaks in the sine wave\n",
    "gamma γ – Ellipticity – Determines how elliptic the 2D bell curve is\n",
    "psi φ – Offset – Defines the phase offset of the sine wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gabor_filter():\n",
    "    \"\"\"This function is designed to produce a set of GaborFilters\n",
    "    \n",
    "    An even distribution of theta values equally distributed amongst pi rad / 180 degree\n",
    "    \"\"\" \n",
    "    filters = []\n",
    "    num_filters = 16\n",
    "    ksize = 35  # The local area to evaluate\n",
    "    sigma = 3.0  # Larger Values produce more edges\n",
    "    _lambda = 10.0\n",
    "    gamma = 0.5\n",
    "    psi = 0  # Offset value - lower generates cleaner results\n",
    "    for theta in np.arange(\n",
    "        0, np.pi, np.pi / num_filters\n",
    "    ):  # Theta is the orientation for edge detection\n",
    "        kern = cv2.getGaborKernel(\n",
    "            (ksize, ksize), sigma, theta, _lambda, gamma, psi, ktype=cv2.CV_64F\n",
    "        )\n",
    "        kern /= 1.0 * kern.sum()  # Brightness normalization\n",
    "        filters.append(kern)\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gabor_filter_to_image(image_path: str, filters, mapping: str = \"bw\"):\n",
    "    \"\"\"This general function is designed to apply filters to our image\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if mapping == \"bw\":\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    elif mapping == \"rgb\":\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        raise (\"Invalid mapping: Valid options are bw or rgb\")\n",
    "\n",
    "    # First create a numpy array the same size as the input image\n",
    "    image_filtered = np.zeros_like(image)\n",
    "\n",
    "    # Starting with a blank image then looping through the images and applying the Gabor Filter\n",
    "    # On each iteration, the highest value is taken until the max value across all filters is obtained\n",
    "    # The final image is returned\n",
    "    depth = -1  # remain depth same as original image\n",
    "\n",
    "    for kern in filters:  # Loop through the kernels in the GaborFilter\n",
    "        image_filter = cv2.filter2D(image, depth, kern)  # Apply filter to image\n",
    "\n",
    "        # Using Numpy.maximum to compare our filter and cumulative image, taking the higher value (max)\n",
    "        np.maximum(image_filtered, image_filter, image_filtered)\n",
    "\n",
    "    return image_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimage(myimage, figsize=[10, 10]):\n",
    "    if (\n",
    "        myimage.ndim > 2\n",
    "    ):  # Does not apply to Black and White images\n",
    "        myimage = myimage[\n",
    "            :, :, ::-1\n",
    "        ]  # OpenCV follows BGR order, while matplotlib likely follows RGB order\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(myimage, cmap=\"gray\", interpolation=\"bicubic\")\n",
    "    plt.xticks([]), plt.yticks([])  # hide tick values on X and Y axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56197245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_canny_edges_image(\n",
    "    image, threshold_1: int = 120, threshold_2: int = 250\n",
    "):\n",
    "    return cv2.Canny(image, threshold1=threshold_1, threshold2=threshold_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_filters = create_gabor_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery_gabor(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.suptitle(\"Gabor Image Filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eadaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated Gabor filters\n",
    "g_filter_titles = [f\"Gabor Filter {i}\" for i in range(0, len(g_filters))]\n",
    "plot_gallery_gabor(g_filters, g_filter_titles, 35, 35, n_row=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge detection\n",
    "g_image = apply_gabor_filter_to_image(seal_image, g_filters, mapping=\"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(apply_canny_edges_image(g_image), cmap=\"inferno\")\n",
    "plt.title(\"Image Edge Detection After Gabor Filters Applied\")\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733412b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_img_width, g_img_height, _ = g_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = cv2.cvtColor(cv2.imread(seal_image), cv2.COLOR_BGR2GRAY)\n",
    "canny_original = apply_canny_edges_image(base_image)\n",
    "canny_gabor = apply_canny_edges_image(g_image)\n",
    "# side_by_side = np.hstack((canny_original, canny_gabor))\n",
    "# showimage(side_by_side,[12,8])\n",
    "\n",
    "def plot_gallery_canny_edges(images, titles, h, w, n_row=1, n_col=2):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "canny_images = [canny_original, canny_gabor]\n",
    "plot_gallery_canny_edges(canny_images, [\"Canny Edges Orginal\", \"Canny Edges Using Gabor Filters\"], g_img_width, g_img_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acb415",
   "metadata": {},
   "source": [
    "# Image Feature Matching\n",
    "\n",
    "*PARAPHRASE BETTER* Feature matching refers to finding corresponding features from two similar images based on a search distance algorithm - where the first image is the source and the second image is the target. [1] The accuracy of feature matching depends on multiple factors like image similarity, complexity, and quality. For feature detection we decided to use the SIFT (scale-invariant feature transform) algorithm [2] which utilizes keypoint localization and nearest neighbor search methods.\n",
    "\n",
    "[1]  \"Feature Matching\", OpenCV\n",
    "Source: https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html \n",
    "\n",
    "[2] \"SIFT\", Wikipedia\n",
    "Source: https://en.wikipedia.org/wiki/Scale-invariant_feature_transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"penguin\"\n",
    "\n",
    "img1 = cv2.cvtColor(cv2.imread(df[df[\"labels\"] == label][\"images\"].iloc[0]), cv2.COLOR_BGR2GRAY)\n",
    "list_of_labeled_images = df[df[\"labels\"] == label][\"images\"][1:400]\n",
    "matchesMask = []\n",
    "finalMask = []\n",
    "most_matches = 0\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "for _, image in enumerate(list_of_labeled_images):\n",
    "    img2 = cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "    \n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "    # Need to draw only good matches, so create a mask\n",
    "    matchesMask = [[0, 0] for i in range(len(matches))]\n",
    "\n",
    "    for i, (m, n) in enumerate(matches):\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            matchesMask[i] = [1, 0]\n",
    "\n",
    "    current_match_count = sum([1 for l in matchesMask for v in l if v == 1])\n",
    "    if current_match_count > most_matches:\n",
    "        most_matches = current_match_count\n",
    "        finalMask = matchesMask\n",
    "\n",
    "draw_params = dict(matchColor = (0, 255, 0),\n",
    "                singlePointColor = (255, 0, 0),\n",
    "                matchesMask = finalMask,\n",
    "                flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "\n",
    "img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, **draw_params)\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9df29",
   "metadata": {},
   "source": [
    "*PARAPHRASE BETTER* feature matching works especially well on mirror images or subjects at different angles. However, feature matching seems to fall short when images included a \"noisy\" background such as the water surface. We speculate that waves/ripples may cause more noise leading to fewer matches in image comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7bd7a3",
   "metadata": {},
   "source": [
    "# Using Machine Learning for Image Classification\n",
    "\n",
    "FastAI's vision learner function to train over the data - after which we show the predictions it made for several images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0293ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(bs, size):\n",
    "    \"\"\"DataBlock Builder for loading data into learning model.\"\"\"\n",
    "    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                       get_items = get_image_files,\n",
    "                       get_y = parent_label,\n",
    "                       splitter = RandomSplitter(),\n",
    "                       item_tfms = Resize(size)\n",
    "                      )\n",
    "    return dblock.dataloaders(path, bs = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de169799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(16, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a35dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, models.resnet34, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08684389",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls = get_dls(16, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a096ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(1, base_lr=9e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 9e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=20, figsize=(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = Interpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd82675",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation.plot_top_losses(k=6, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc75856",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ebd6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
