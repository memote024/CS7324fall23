{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045a47fe",
   "metadata": {},
   "source": [
    "# Lab Assignment Six: Convolutional Network Architectures\n",
    "\n",
    "## Catherine Magee, Morgan Mote, Luv Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea8894",
   "metadata": {},
   "source": [
    "In this lab, you will select a prediction task to perform on your dataset, evaluate a deep learning architecture and tune hyper-parameters. If any part of the assignment is not clear, ask the instructor to clarify. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "### Dataset Selection\n",
    "\n",
    "Select a dataset identically to lab two (images). That is, the dataset must be image data. In terms of generalization performance, it is helpful to have a large dataset of identically sized images. It is fine to perform binary classification or multi-class classification. You are not allowed to use MNIST, Fashion, MNIST, or the sklearn digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0470045",
   "metadata": {},
   "source": [
    "##### Grading Rubric\n",
    "\n",
    "- Preparation (3 points total)  \n",
    "\n",
    ">[1.5 points] Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    ">[1.5 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "- Modeling (6 points total)\n",
    "\n",
    ">[1.5 points]  Setup the training to use data expansion in Keras (also called data augmentation). Explain why the chosen data expansion techniques are appropriate for your dataset. You should make use of Keras augmentation layers, like in the class examples.\n",
    "\n",
    ">[2 points] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures and investigate changing one or more parameters of each architecture such as the number of filters. This means, at a  minimum, you will train a total of four models (2 different architectures, with 2 parameters changed in each architecture). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras). Be sure that models converge. \n",
    "\n",
    ">[1.5 points] Visualize the final results of all the CNNs and interpret/compare the performances. Use proper statistics as appropriate, especially for comparing models. \n",
    "\n",
    ">[1 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  \n",
    "\n",
    "- Exceptional Work (1 points total)\n",
    "\n",
    ">You have free reign to provide additional analyses. One idea (required for 7000 level students): Use transfer learning with pre-trained weights for your initial layers of your CNN. Compare the performance when using transfer learning to your best model from above in terms of classification performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e752592",
   "metadata": {},
   "source": [
    "##### CNN Rubric\n",
    "\n",
    "- Metric Chosen\n",
    "> Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "- Data Separation\n",
    ">Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice.\n",
    "\n",
    "- Create CNN and Use Expansion\n",
    ">Create a convolutional neural network to use on your data using Keras. Setup the training to use data expansion in Keras. Explain why the chosen data expansion techniques are appropriate for your dataset.\n",
    "\n",
    "- Build Two Different Architectures\n",
    ">Investigate at least two different convolutional network architectures (and investigate changing some parameters of each architecture). Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras).\n",
    "\n",
    "- Visualize and Compare\n",
    ">Visualize the final results of the CNNs and interpret the performance. Use proper statistics as appropriate, especially for comparing models.\n",
    "\n",
    "- Compare to MLP\n",
    ">Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.\n",
    "\n",
    "- Exceptional Work\n",
    ">Required for 7000 level students: Use transfer learning to pre-train the weights of your initial layers of your CNN. Compare the performance when using transfer learning to training from scratch in terms of classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd29fc",
   "metadata": {},
   "source": [
    "### Dataset Selection:\n",
    "\n",
    "https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "\n",
    "##### Content\n",
    "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
    "\n",
    "Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care.\n",
    "\n",
    "For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n",
    "\n",
    "The normal chest X-ray depicts clear lungs without any areas of abnormal opacification in the image. Bacterial pneumonia  typically exhibits a \"focal lobar consolidation\", whereas viral pneumonia manifests with a more \"diffuse interstitial pattern\" in both lungs.\n",
    "\n",
    "##### Acknowledgements\n",
    "Data: https://data.mendeley.com/datasets/rscbjbr9sj/2\n",
    "\n",
    "License: CC BY 4.0\n",
    "\n",
    "Citation: http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0232e6ba",
   "metadata": {},
   "source": [
    "# DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1010548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\morga\\anaconda3\\lib\\site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.14.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.24.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.14.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c66e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.22.0-cp39-cp39-win_amd64.whl (729 kB)\n",
      "     ------------------------------------- 729.9/729.9 kB 11.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\morga\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.3)\n",
      "Collecting typeguard<3.0.0,>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\morga\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.9)\n",
      "Installing collected packages: typeguard, tensorflow_addons\n",
      "Successfully installed tensorflow_addons-0.22.0 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d409fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from IPython.display import display, HTML, Markdown, clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import (\n",
    "    utils,\n",
    "    models,\n",
    "    layers,\n",
    "    metrics,\n",
    "    preprocessing,\n",
    "    callbacks,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Reshape,\n",
    "    Input,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Activation,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    average,\n",
    "    Add,\n",
    "    concatenate,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    make_scorer,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def clear_screen():\n",
    "    time.sleep(2)\n",
    "    print(\"Clearing screen...\")\n",
    "    time.sleep(2)\n",
    "    clear_output()\n",
    "\n",
    "clear_screen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9dd6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_GPU_MEMORY = False\n",
    "\n",
    "if LIMIT_GPU_MEMORY:\n",
    "    # Limit GPU Memory Usage\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(\n",
    "        physical_devices[0],\n",
    "        False\n",
    "    )\n",
    "    os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = str(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f131d",
   "metadata": {},
   "source": [
    "### Evaluation Method\n",
    "\n",
    "For evaluating our model's performance, we will be using recall. Between the two conditions, pneumonia is potentially very dangerous for the young, elderly, and people already diagnosed with a disease or respiratory condition. We want to consider false negatives (the model predicts pneumonia lesions when it is actually something else like bronchiectasis) as we evaluate whether our model is performing well or not.\n",
    "\n",
    "Beyond recall, we use metrics such as accuracy, misclassificaiton, precision, recall, and F1-score to measure and compare algorithm performance.\n",
    "\n",
    "- Accuracy is a measure of the overall correctness of a model's predictions, calculated as the ratio of the number of correct predictions to the total number of predictions.\n",
    "\n",
    "- Misclass, or misclassification, refers to the instances where a model's prediction does not match the actual class label of a data point.\n",
    "\n",
    "- Precision is a measure of how well a model correctly identifies positive instances among the predicted positive instances.\n",
    "\n",
    "- Recall, also known as sensitivity or the true positive rate, is a measure of how well a model identifies all the positive instances among the actual positive instances.\n",
    "\n",
    "- F1-score is the \"harmonic mean\" of precision and recall, providing a single value that balances the trade-off between precision and recall.\n",
    "\n",
    "- Additionally, we graph the values of Loss and AUC (Area Under Curve) for comparisons.\n",
    "\n",
    "We want to consider all of these metrics since not only do we want our models to perform well, but we want to achieve the highest true positive rate for both classes as possible. Given the importance of recognizing false positive and negatives, we need to measure and quantify the performance our our models by all of these metrics.\n",
    "\n",
    "### Dataset Splitting\n",
    "\n",
    "We have around 5900 observations, so the amount of data is not a concern. We also assessed the distribution of the class variable and found that the target class variable is evenly distributed.\n",
    "\n",
    "We feel that the data is already well-structured and there is not a need to split the data into subsets or \"folds\". The training and testing folders appear to be representative of the overall population of images that the model is expected to encounter.\n",
    "\n",
    "We have decided to proceed with the current testing and training datasets as they are, without performing cross-validation. We feel that not using cross-validation can be justified for a few reasons:\n",
    "\n",
    "For one, the dataset size is large enough and representative of the overall population of images, so it may not be necessary to perform cross-validation as the model can learn from the full dataset without having to split it into subsets. Additionally, as the training and testing folders have a balanced distribution of images across all classes, then cross-validation may not be necessary as the model canlearn again from the full dataset without having to split it into subsets.\n",
    "\n",
    "We additonally decided that it would be acceptable to use grayscale versions of the images for the CNN models, since x-ray images are not depicted as RGB to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96c64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6820ea",
   "metadata": {},
   "source": [
    "The presented code undertakes a series of tasks to process and prepare image datasets for machine learning:\n",
    "\n",
    "Initially, it loads images categorized as \"NORMAL\" and \"PNEUMONIA,\" returning normalized and resized versions. This functionality operates by accepting a directory path pointing to the image folder and generating two lists: one containing the images and another containing their corresponding class labels.\n",
    "\n",
    "Subsequently, the code combines the training and testing datasets into two arrays, segregating images and labels.\n",
    "\n",
    "To provide a visual representation of the dataset's class distribution, a bar chart is generated. Following this, the data undergoes a split into training and testing sets utilizing the train_test_split() function from scikit-learn.\n",
    "\n",
    "The code introduces the plot_gallery() function, designed to exhibit a gallery of images from the training set along with their associated class labels.\n",
    "\n",
    "The script defines two image datasets utilizing TensorFlow's image_dataset_from_directory function. One dataset is allocated for training purposes, while the other serves as the testing dataset. Images in both datasets are resized to dimensions of 64x64 pixels. A preview of images from the training dataset, accompanied by their respective class labels, is then displayed.\n",
    "\n",
    "Additionally, the pixel values of images in both datasets are normalized, ensuring they fall within the 0 to 1 range through the utilization of the normalize function.\n",
    "\n",
    "Lastly, the process_ds_to_numpy function is introduced. This function transforms datasets into numpy arrays for integration into a machine learning model. This is achieved by iteratively traversing each image in the dataset, appending its pixel values to a list, and subsequently concatenating these lists to form numpy arrays. The resulting arrays for the training and testing datasets are denoted as x_train, y_train, x_test, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12314cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 64, 64\n",
    "img_color_mode = \"grayscale\"  # can also be 'rgb'\n",
    "classes = {0: \"NORMAL\", 1: \"PNEUMONIA\"}\n",
    "n_classes = 2\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"./train/\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    color_mode=img_color_mode,\n",
    "    batch_size=32,\n",
    "    image_size=(img_width, img_height),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"./test/\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    color_mode=img_color_mode,\n",
    "    batch_size=32,\n",
    "    image_size=(img_width, img_height),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(25):\n",
    "        ax = plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"), cmap=\"gray\")\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "plt.suptitle(\"Image Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e872b",
   "metadata": {},
   "source": [
    "The provided code comprises functions for normalizing pixel values in images and converting TensorFlow datasets to numpy arrays.\n",
    "\n",
    "The normalize function accepts an image and its label, normalizes the pixel values of the image to a range between 0 and 1, and returns the normalized image along with its label. Subsequently, the train_ds and test_ds datasets undergo normalization by mapping them to the normalize function.\n",
    "\n",
    "Concluding the script, the process_ds_to_numpy function is employed to convert the train_ds and test_ds datasets into numpy arrays. The final line of the code determines the number of dimensions in the resulting x_train numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    \"\"\"Normalize the pixel values of the image to be between 0 and 1.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(normalize)\n",
    "test_ds = test_ds.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c41b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ds_to_numpy(ds) -> tuple:\n",
    "    \"\"\"Returns the x, y numpy arrays from the dataset.\"\"\"\n",
    "    x, y = [], []\n",
    "    for image, label in ds.as_numpy_iterator():\n",
    "        x.append(np.array(image, dtype=np.float32))\n",
    "        y.append(np.array(label, dtype=np.int32))\n",
    "\n",
    "    return np.concatenate(x, axis=0), np.concatenate(y, axis=0)\n",
    "\n",
    "\n",
    "x_train, y_train = process_ds_to_numpy(train_ds)\n",
    "x_test, y_test = process_ds_to_numpy(test_ds)\n",
    "\n",
    "n_dimensions = x_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15763f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178eb781",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "\n",
    "The ImageDataGenerator function is employed to dynamically apply diverse image transformations during model training, encompassing rotations, zooms, shifts, and flips. This generator is then configured to the training and test data through the fit method.\n",
    "\n",
    "Introducing additional image transformations to the existing data serves to augment the size of our training dataset. Leveraging rotation, zoom, shift, and flip techniques exposes our models to varied image angles, enhancing their ability to make predictions based on both the original and augmented data. In the context of our citrus disease dataset, where diseases exhibit defining traits but manifest in numerous variations with distinct physical symptoms, generating images from existing ones facilitates the creation of diverse disease variations. This approach not only enriches the training data but also mitigates the risk of overfitting, ensuring our models are better equipped to generalize to various manifestations of the diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = preprocessing.image.ImageDataGenerator(\n",
    "       rotation_range = 20, \n",
    "       width_shift_range=0.1,  \n",
    "       height_shift_range=0.1,  \n",
    "       horizontal_flip = True,  \n",
    "       vertical_flip = False,\n",
    ")\n",
    "# Fits the data to the generator.\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encodes the inputs\n",
    "# One-hot encoding is necessary for the model to output probabilities for each class during training and evaluation.\n",
    "y_train_ohe = utils.to_categorical(y_train, n_classes)\n",
    "y_test_ohe = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reshapes the arrays to have a sample size of -1, and an image size of 64x64 with one channel, and \n",
    "then expands the # dimensions of the arrays to include the channel dimension, resulting in a final shape \n",
    "of (samples, image_rows, image_cols, image_channels)\"\"\" \n",
    "\n",
    "for tmp in datagen.flow(x_train, y_train_ohe, batch_size=1):\n",
    "    plt.imshow(cv2.resize(tmp[0].squeeze(), (64, 64)), cmap='gray')\n",
    "    plt.title(\"Generated X-Ray\")\n",
    "    plt.axis(\"off\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e2aa7",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "In the following code segment, we establish a flow iterator by utilizing the previously defined image data generator object to produce augmented images in batches of size one from the training set. Subsequently, we employ matplotlib to visualize one of the augmented images.\n",
    "\n",
    "To enhance visualization and gain insights into the augmented data, we specifically generate plots for 25 sample images from the training set. This allows us to observe the variations introduced through augmentation, providing a comprehensive view of the diverse transformations applied to the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6024114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    cm,\n",
    "    target_names,\n",
    "    title=\"Confusion matrix\",\n",
    "    cmap=None,\n",
    "    normalize=True,\n",
    "    class_results: dict = {},\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap(\"Blues\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:0.4f}\".format(cm[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "        else:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:,}\".format(cm[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    if class_results:\n",
    "        x_lab = \"Predicted label\\n\\n\"\n",
    "        x_lab += f\"accuracy={accuracy:0.4f}\\nmisclass={misclass:0.4f}\\n\"\n",
    "        for key, value in class_results.items():\n",
    "            x_lab += f\"{key}={value:0.4f}\\n\"\n",
    "    else:\n",
    "        x_lab = \"Predicted label\\n\\n\"\n",
    "        x_lab += f\"accuracy={accuracy:0.4f}\\nmisclass={misclass:0.4f}\\n\"\n",
    "        \n",
    "    plt.xlabel(x_lab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4106661",
   "metadata": {},
   "source": [
    "In the code below, a helper function named compare_mlp_cnn is introduced. This function takes the following inputs:\n",
    "\n",
    "- cnn and mlp: the trained convolutional neural network and multilayer perceptron models, respectively.\n",
    "- X_test and y_test: the testing data and corresponding labels.\n",
    "- labels: an optional list of class labels, defaulting to 'auto'.\n",
    "\n",
    "The primary purpose of this function is to assess and compare the performance of the CNN and MLP models on the test data. It achieves this by making predictions for class labels and subsequently computing accuracy scores and confusion matrices. The results are then visualized using a heatmap, leveraging the seaborn library. This visualization aids in providing an intuitive representation of the comparative performance between the two models on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_mlp_cnn(model_1, model_2, X_test, y_test, title_1: str, title_2: str, labels='auto'):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    if model_1 is not None:\n",
    "        yhat_model_1 = np.argmax(model_1.predict(X_test), axis=1)\n",
    "        acc_model_1 = metrics.accuracy_score(y_test, yhat_model_1)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        cm = metrics.confusion_matrix(y_test, yhat_model_1)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2%', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f\"{title_1}: {acc_model_1:.3f}\")\n",
    "    \n",
    "    if model_2 is not None:\n",
    "        yhat_model_2 = np.argmax(model_2.predict(X_test), axis=1)\n",
    "        acc_model_2 = metrics.accuracy_score(y_test, yhat_model_2)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cm = metrics.confusion_matrix(y_test,yhat_model_2)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2%', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f\"{title_2}: {acc_model_2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b48b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the performance of a model during training and identifying overfitting or underfitting.\n",
    "def plot_history(history):\n",
    "    # plot history of the model:\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss over epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc183a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The summarize_net helper function is designed to provide a comprehensive summary of a neural network \n",
    "model's performance. This function takes the following inputs:\n",
    "\n",
    "- net: the neural network model.\n",
    "- x_test: the test set features.\n",
    "- y_test: the test set labels.\n",
    "- An optional string parameter for the title of the plot.\n",
    "\n",
    "The function utilizes the predict method of the model to generate predictions for the test set. It then \n",
    "computes the accuracy score by comparing these predicted labels with the actual labels. Additionally, \n",
    "the function calculates the confusion matrix, normalizes it, and presents it as aheatmap using the Seaborn \n",
    "library. The title of the plot is constructed to include the accuracy score formatted to four decimal places, \n",
    "along with any optional title text provided to the function. This visual representation aids in effectively \n",
    "summarizing the model's performance on the given test set.\"\"\"\n",
    "\n",
    "def summarize_net(net, x_test, y_test, title_text=\"\"):\n",
    "    label = [\"NORMAL\",\"PNEUMONIA\"]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    yhat = np.argmax(net.predict(x_test), axis=1)\n",
    "    acc = metrics.accuracy_score(y_test, yhat)\n",
    "    cm = metrics.confusion_matrix(y_test, yhat)\n",
    "    cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2%\", xticklabels=label, yticklabels=label)\n",
    "    plt.title(title_text + \"{:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542e95c",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "The provided code defines a 3-layer perceptron (MLP) model using Keras with the following specifications:\n",
    "\n",
    "- The input images are flattened before being fed into the MLP.\n",
    "- The first hidden layer has 30 units with a Rectified Linear Unit (ReLU) activation function.\n",
    "- The second hidden layer has 15 units, also with a ReLU activation function.\n",
    "- The output layer has as many units as the number of classes, employing a softmax activation function.\n",
    "- The model is compiled with the RMSprop optimizer and mean squared error loss function.\n",
    "- Evaluation metrics include Recall and Area Under the Curve (AUC).\n",
    "- Training is performed on the training data (x_train and y_train_ohe) with a batch size of 50 and 250 epochs.\n",
    "- The training history is saved in the variable model_history.\n",
    "- Finally, the model summary is printed.\n",
    "\n",
    "This model architecture and configuration are suitable for a classification task, and the choice of metrics indicates a focus on assessing both sensitivity (Recall) and overall discriminative performance (AUC). The code provides a comprehensive overview of the model's architecture, training parameters, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3489bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a 3-layer Keras MLP\n",
    "mlp = models.Sequential()\n",
    "\n",
    "# Makes the images flat for the MLP input\n",
    "mlp.add(layers.Flatten())\n",
    "mlp.add(layers.Dense(input_dim=1, units=30, activation=\"relu\"))\n",
    "mlp.add(layers.Dense(units=15, activation=\"relu\"))\n",
    "mlp.add(layers.Dense(n_classes))\n",
    "mlp.add(layers.Activation(\"softmax\"))\n",
    "\n",
    "# Compiles the model\n",
    "mlp.compile(\n",
    "    optimizer=\"Adamax\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "# Fits the model to the training data\n",
    "mlp_history = mlp.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    batch_size=50,\n",
    "    epochs=250,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the visualization of the MLP model in a PNG image file\n",
    "utils.plot_model(\n",
    "    mlp,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"LR\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e05e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the performance of an MLP model on a test set by predicting the class \n",
    "# probabilities and rounding them to obtain class predictions.\n",
    "\n",
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba = mlp.predict(x_test)\n",
    "y_predict = np.round(y_predict_proba)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a43736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the loss over epochs\n",
    "epochs = mlp_history.epoch\n",
    "loss = mlp_history.history[\"loss\"]\n",
    "\n",
    "# Plots the loss graph\n",
    "plt.plot(epochs, loss)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae496759",
   "metadata": {},
   "source": [
    "##### Observing the performance of the MLP with the dataset, the curve appears to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The provided code evaluates and visualizes the Receiver Operating Characteristic (ROC) curve and \n",
    "Area Under the Curve (AUC) for the MLP model. The ROC curve is a graphical representation illustrating \n",
    "the performance of a binary classifier system by displaying the trade-off between the true positive rate (TPR) \n",
    "and the false positive rate (FPR) as the decision threshold for classification varies.\n",
    "\n",
    "In this run, the calculated AUC is 0.95, indicating that the model possesses a strong ability to distinguish \n",
    "between positive and negative cases. The ROC curve visually supports this interpretation by depicting a steep \n",
    "rise in the true positive rate while maintaining a low false positive rate. This pattern suggests that the model \n",
    "effectively identifies positive cases while minimizing false positives. Overall, the high AUC and the shape of the \n",
    "ROC curve imply that the MLP model performs well on the test set, demonstrating its capability to accurately classify \n",
    "the data.\"\"\"\n",
    "\n",
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the training versus testing graph for the MLP model\n",
    "# Model history values\n",
    "hist_values = list(mlp_history.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150d120",
   "metadata": {},
   "source": [
    "##### The graph has three subplots: the first subplot shows the training and validation loss versus epochs, the second subplot shows the training and validation recall versus epochs, and the third subplot shows the training and validation AUC versus epochs. \n",
    "\n",
    "The training versus testing graph is used to visualize the performance of the model during training. In this case, we can see that the training loss and validation loss INCREASE/DECREASE as the number of epochs INCREASE/DECREASE, indicating that the model is ***learning and improving***. The training recall and validation recall also INCREASE/DECREASE as the number of epochs INCREASE/DECREASE, indicating that the model is becoming BETER/WORSE at identifying positive cases. Finally, the training AUC and validation AUC INCREASE/DECREASE as the number of epochs INCREASE/DECREASE, indicating that the model is becoming BETER/WORSE at distinguishing between positive and negative cases.\n",
    "\n",
    "Overall, the graph shows that the MLP model is BETTER/WORSE over time, and that it is performing GOOD/BAD on both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21900f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict, output_dict=True\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850c12e",
   "metadata": {},
   "source": [
    "The provided code generates a confusion matrix and visualizes it using the plot_confusion_matrix function. The confusion matrix provides a detailed breakdown of the number of true positives, true negatives, false positives, and false negatives for each class. In this specific context, the matrix illustrates the correct and incorrect classifications of images for Citrus Canker and Black Spot diseases.\n",
    "\n",
    "From the confusion matrix, it is evident that the MLP model correctly predicted \n",
    "\n",
    "However, there were ### instances where NORMAL/PNEUMINOA was incorrectly predicted when the true class was \n",
    "\n",
    "***While the overall performance of the MLP model is good in classifying the two classes, there is a slight opportunity for improvement, particularly in reducing the number of false positives and false negatives.***\n",
    "\n",
    "Analyzing the confusion matrix in this manner provides valuable insights into the model's strengths and areas for enhancement, aiding in the refinement of the classification algorithm.\n",
    "\n",
    "Accuracy is a measure of the overall correctness of a model's predictions, calculated as the ratio of the number of correct predictions to the total number of predictions. For this run, our initial accuracy score is \n",
    "\n",
    "Misclass, or misclassification, refers to the instances where a model's prediction does not match the actual class label of a data point. Misclassification can be measured using various metrics such as accuracy, precision, recall, and F1-score - but in this case we can easily determine misclass by calculating 1.0 minus the accuracy score. In our case, we observe the misclass as \n",
    "\n",
    "Precision is a measure of how well a model correctly identifies positive instances among the predicted positive instances. Precision is calculated as the ratio of the number of true positive predictions to the total number of positive predictions (which would be the true positive predictions plus the false positive predictions). In our case, the precision score is \n",
    "\n",
    "Recall, also known as sensitivity or the true positive rate, is a measure of how well a model identifies all the positive instances among the actual positive instances. Recall is calculated as the ratio of the number of true positive predictions to the total number of actual positive instances (i.e., sum of true positive and false negative predictions). Our recall score for this run was \n",
    "\n",
    "F1-score is the \"harmonic mean\" of precision and recall, providing a single value that balances the trade-off between precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall). Our F1-score for this run was "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c6044",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "### CNN 1\n",
    "\n",
    "The provided code constructs a Convolutional Neural Network (CNN) named cnn1 with the following architecture:\n",
    "\n",
    "[1] Convolutional Layers:\n",
    "- Two convolutional layers with ReLU activation.\n",
    "- Max-pooling layers follow each convolutional layer.\n",
    "\n",
    "[2] Flattening:\n",
    "- The output from the convolutional layers is flattened.\n",
    "\n",
    "[3] Dense Layers:\n",
    "- A dense layer with 100 units and ReLU activation on the flattened output.\n",
    "- Another dense layer with 100 units and ReLU activation.\n",
    "- A final dense layer with softmax activation, producing predicted probabilities for each class.\n",
    "\n",
    "[4] Compilation:\n",
    "- The model is compiled with mean squared error as the loss function.\n",
    "- RMSprop is used as the optimizer.\n",
    "\n",
    "[5] Training:\n",
    "- The model is trained on the training data with a batch size of 50.\n",
    "- Training occurs for 150 epochs, with validation data provided for monitoring performance during training.\n",
    "\n",
    "This architecture is common for image classification tasks using CNNs. The use of convolutional layers helps the model learn hierarchical features from the input images, and the subsequent dense layers facilitate the final classification. The choice of activation functions, ReLU in this case, is standard for introducing non-linearity. The softmax activation in the final layer is appropriate for multi-class classification, producing probability distributions across different classes.\n",
    "\n",
    "The training is performed over 150 epochs, and validation data is provided to assess the model's generalization performance during training. The choice of mean squared error as the loss function might be more common in regression tasks, and for classification tasks, categorical crossentropy is often used. Additionally, the choice of RMSprop as the optimizer is suitable for many CNN applications. Depending on the specific problem and dataset, further adjustments to these parameters might be considered for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator to feed the CNN model cnn1 with augmented data during the training process\n",
    "# Creates a CNN with convolution layer and max pooling\n",
    "cnn1 = models.Sequential()\n",
    "cnn1.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn1.add(Activation(\"relu\"))\n",
    "cnn1.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "    )\n",
    ")\n",
    "cnn1.add(Activation(\"relu\"))\n",
    "cnn1.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adds 1 layer on flattened output\n",
    "cnn1.add(Flatten())\n",
    "cnn1.add(Dense(100, activation=\"relu\"))\n",
    "cnn1.add(Dense(50, activation=\"relu\"))\n",
    "cnn1.add(Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compiles the model\n",
    "cnn1.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "# Fits the model to training data\n",
    "history1 = cnn1.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a CNN with convolution layer and max pooling for use with flow generator.\n",
    "cnn1_flow = models.Sequential()\n",
    "cnn1_flow.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn1_flow.add(Activation(\"relu\"))\n",
    "cnn1_flow.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "    )\n",
    ")\n",
    "cnn1_flow.add(Activation(\"relu\"))\n",
    "cnn1_flow.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adds 1 layer on flattened output\n",
    "cnn1_flow.add(Flatten())\n",
    "cnn1_flow.add(Dense(100, activation=\"relu\"))\n",
    "cnn1_flow.add(Dense(50, activation=\"relu\"))\n",
    "cnn1_flow.add(Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compiles the model\n",
    "cnn1_flow.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "# Fits the model to training data using flow generator\n",
    "history1_1 = cnn1_flow.fit(\n",
    "    datagen.flow(x_train, y_train_ohe, batch_size=128),\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    epochs=150,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10, start_from_epoch=100\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn1_flow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the graph\n",
    "utils.plot_model(\n",
    "    cnn1,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"LR\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b66aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba = cnn1.predict(x_test)\n",
    "y_predict = np.round(y_predict_proba)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba_flow = cnn1_flow.predict(x_test)\n",
    "y_predict_flow = np.round(y_predict_proba_flow)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87551b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the loss over epochs\n",
    "epochs = history1.epoch\n",
    "loss = history1.history[\"loss\"]\n",
    "\n",
    "epochs_flow = history1_1.epoch\n",
    "loss_flow = history1_1.history[\"loss\"]\n",
    "\n",
    "# Plots the loss graph\n",
    "plt.plot(epochs, loss, label=\"CNN\")\n",
    "plt.plot(epochs_flow, loss_flow, label=\"CNN (Flow Generator) w/ Early Stopping\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict, output_dict=True\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict_flow, output_dict=True\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict_flow, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN (With Flow Generator) Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 1 - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba_flow, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 1 (Flow Generator) - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history1.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 1\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history1_1.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 1 With Flow Generator\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dec4c3",
   "metadata": {},
   "source": [
    "# CNN 1 vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn1,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 1\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn1_flow,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 1 (Flow Generator)\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e645a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn1,\n",
    "    cnn1_flow,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 1\",\n",
    "    title_2=\"CNN 1 (Flow Generator)\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17af5a8",
   "metadata": {},
   "source": [
    "### CNN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a CNN with convolution layer and max pooling\n",
    "cnn2 = models.Sequential()\n",
    "cnn2.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "    )\n",
    ")\n",
    "cnn2.add(\n",
    "    layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "    )\n",
    ")\n",
    "cnn2.add(Activation(\"relu\"))\n",
    "cnn2.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        data_format=\"channels_last\"\n",
    "    )\n",
    ")\n",
    "cnn2.add(Activation(\"relu\"))\n",
    "cnn2.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        data_format=\"channels_last\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adds dropout layer\n",
    "cnn2.add(Dropout(0.2))\n",
    "\n",
    "# Adds 1 layer on flattened output\n",
    "cnn2.add(Flatten())\n",
    "\n",
    "cnn2.add(Dense(512, activation=\"relu\"))\n",
    "cnn2.add(Dense(256, activation=\"relu\"))\n",
    "cnn2.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Adds dropout layer\n",
    "cnn2.add(Dropout(0.4))\n",
    "\n",
    "cnn2.add(Dense(n_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Compiles the model\n",
    "cnn2.compile(\n",
    "    optimizer=\"Adamax\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "# Fits the model to training data\n",
    "history2 = cnn2.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, start_from_epoch=50\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a CNN with convolution layer and max pooling for use with flow generator.\n",
    "cnn2_flow = models.Sequential()\n",
    "cnn2_flow.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "    )\n",
    ")\n",
    "cnn2_flow.add(\n",
    "    layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "    )\n",
    ")\n",
    "cnn2_flow.add(Activation(\"relu\"))\n",
    "cnn2_flow.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "cnn2_flow.add(Activation(\"relu\"))\n",
    "cnn2_flow.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "# Adds dropout layer\n",
    "cnn2_flow.add(Dropout(0.2))\n",
    "\n",
    "# Adds 1 layer on flattened output\n",
    "cnn2_flow.add(Flatten())\n",
    "\n",
    "# cnn2_flow.add(Dense(1024, activation=\"relu\"))\n",
    "cnn2_flow.add(Dense(512, activation=\"relu\"))\n",
    "cnn2_flow.add(Dense(256, activation=\"relu\"))\n",
    "cnn2_flow.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Adds dropout layer\n",
    "cnn2_flow.add(layers.Dropout(0.4))\n",
    "\n",
    "cnn2_flow.add(Dense(n_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Compiles the model\n",
    "cnn2_flow.compile(\n",
    "    optimizer=\"Adamax\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "# Fits the model to training data using flow generator\n",
    "history2_2 = cnn2_flow.fit(\n",
    "    datagen.flow(x_train, y_train_ohe, batch_size=50),\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    epochs=150,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3,\n",
    "            start_from_epoch=50,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn2_flow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the graph\n",
    "utils.plot_model(\n",
    "    cnn2,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"LR\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dffdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba = cnn2.predict(x_test)\n",
    "y_predict = np.round(y_predict_proba)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba_flow = cnn2_flow.predict(x_test)\n",
    "y_predict_flow = np.round(y_predict_proba_flow)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict_flow, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the loss over epochs\n",
    "epochs = history2.epoch\n",
    "loss = history2.history[\"loss\"]\n",
    "\n",
    "epochs_flow = history2_2.epoch\n",
    "loss_flow = history2_2.history[\"loss\"]\n",
    "\n",
    "# Plots the loss graph\n",
    "plt.plot(epochs, loss, label=\"CNN w/ Early Stopping\")\n",
    "plt.plot(epochs_flow, loss_flow, label=\"CNN (Flow Generator) w/ Early Stopping\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict, output_dict=True\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict_flow, output_dict=True, zero_division=0\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict_flow, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN (With Flow Generator) Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac33e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 2 - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9974167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba_flow, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 2 (Flow Generator) - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history2.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history2_2.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 2 With Flow Generator\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c12cf",
   "metadata": {},
   "source": [
    "# CNN 2 vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn2,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 2\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4697864",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn2_flow,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 2 (Flow Generator)\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn2,\n",
    "    cnn2_flow,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 2\",\n",
    "    title_2=\"CNN 2 (Flow Generator)\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e0a43",
   "metadata": {},
   "source": [
    "### CNN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kaiming He to regularize ReLU layers: https://arxiv.org/pdf/1502.01852.pdf\n",
    "# Use Glorot/Bengio for linear/sigmoid/softmax: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "l2_lambda = 1e-4\n",
    "cnn3 = Sequential()\n",
    "\n",
    "cnn3.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn3.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "cnn3.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn3.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        input_shape=(img_width, img_width, n_dimensions),\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn3.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "cnn3.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn3.add(\n",
    "    Conv2D(\n",
    "        filters=128,\n",
    "        input_shape=(img_width, img_width, n_dimensions),\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")  # more compact syntax\n",
    "\n",
    "cnn3.add(\n",
    "    Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn3.add(Flatten())\n",
    "cnn3.add(Dropout(0.25))  # add some dropout for regularization after conv layers\n",
    "cnn3.add(\n",
    "    Dense(\n",
    "        128,\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "    )\n",
    ")\n",
    "cnn3.add(Dropout(0.5))  # add some dropout for regularization, again!\n",
    "cnn3.add(\n",
    "    Dense(\n",
    "        n_classes,\n",
    "        activation=\"sigmoid\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Let's train the model\n",
    "cnn3.compile(\n",
    "    optimizer=\"Adamax\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "history3 = cnn3.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3,\n",
    "            start_from_epoch=50,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kaiming He to regularize ReLU layers: https://arxiv.org/pdf/1502.01852.pdf\n",
    "# Use Glorot/Bengio for linear/sigmoid/softmax: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "l2_lambda = 1e-4\n",
    "cnn3_flow = Sequential()\n",
    "\n",
    "cnn3_flow.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn3_flow.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "cnn3_flow.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn3_flow.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        input_shape=(img_width, img_width, n_dimensions),\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn3_flow.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "cnn3_flow.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn3_flow.add(\n",
    "    Conv2D(\n",
    "        filters=128,\n",
    "        input_shape=(img_width, img_width, n_dimensions),\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn3_flow.add(\n",
    "    Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        data_format=\"channels_last\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn3_flow.add(Flatten())\n",
    "cnn3_flow.add(Dropout(0.25))  # add some dropout for regularization after conv layers\n",
    "cnn3_flow.add(\n",
    "    Dense(\n",
    "        128,\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "    )\n",
    ")\n",
    "cnn3_flow.add(Dropout(0.5))  # add some dropout for regularization, again!\n",
    "cnn3_flow.add(\n",
    "    Dense(\n",
    "        n_classes,\n",
    "        activation=\"softmax\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=l2(l2_lambda),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Let's train the model\n",
    "cnn3_flow.compile(\n",
    "    optimizer=\"Adamax\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "history3_3 = cnn3_flow.fit(\n",
    "    datagen.flow(x_train, y_train_ohe, batch_size=128),\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3,\n",
    "            start_from_epoch=50,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the graph\n",
    "utils.plot_model(\n",
    "    cnn3,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"LR\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba = cnn3.predict(x_test)\n",
    "y_predict = np.round(y_predict_proba)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4109376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba_flow = cnn3_flow.predict(x_test)\n",
    "y_predict_flow = np.round(y_predict_proba_flow)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict_flow, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the loss over epochs\n",
    "epochs = history3.epoch\n",
    "loss = history3.history[\"loss\"]\n",
    "\n",
    "epochs_flow = history3_3.epoch\n",
    "loss_flow = history3_3.history[\"loss\"]\n",
    "\n",
    "# Plots the loss graph\n",
    "plt.plot(epochs, loss, label=\"CNN w/ Early Stopping\")\n",
    "plt.plot(epochs_flow, loss_flow, label=\"CNN (Flow Generator) w/ Early Stopping\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict, output_dict=True\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9eef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict_flow, output_dict=True, zero_division=0\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict_flow, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN (With Flow Generator) Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fe1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 3 - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba_flow, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 3 (Flow Generator) - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history3.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 3 With Flow Generator\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history3_3.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 3 With Flow Generator\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af0fe0",
   "metadata": {},
   "source": [
    "# CNN 3 vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn3,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 3\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn3_flow,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 3 (Flow Generator)\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn3,\n",
    "    cnn3_flow,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 3\",\n",
    "    title_2=\"CNN 3 (Flow Generator)\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930896d0",
   "metadata": {},
   "source": [
    "### CNN 4: Using a LeNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f515acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_holder = Input(shape=(img_width, img_height, n_dimensions))\n",
    "\n",
    "# start with a conv layer\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    input_shape=(img_width, img_height, n_dimensions),\n",
    "    kernel_size=(3, 3),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(input_holder)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x)\n",
    "\n",
    "x_split = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x_split)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(1, 1),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x)\n",
    "\n",
    "# now add back in the split layer, x_split (residual added in)\n",
    "x = Add()([x, x_split])\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(256)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(n_classes)(x)\n",
    "x = Activation(\"softmax\")(x)\n",
    "\n",
    "cnn4 = Model(inputs=input_holder, outputs=x)\n",
    "\n",
    "cnn4.compile(\n",
    "    optimizer=\"Adamax\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "history4 = cnn4.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3,\n",
    "            start_from_epoch=50,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_holder = Input(shape=(img_width, img_height, n_dimensions))\n",
    "\n",
    "# start with a conv layer\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    input_shape=(img_width, img_height, n_dimensions),\n",
    "    kernel_size=(3, 3),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(input_holder)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x)\n",
    "\n",
    "x_split = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x_split)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(1, 1),\n",
    "    kernel_initializer=\"he_uniform\",\n",
    "    kernel_regularizer=l2(l2_lambda),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    ")(x)\n",
    "\n",
    "# now add back in the split layer, x_split (residual added in)\n",
    "x = Add()([x, x_split])\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(256)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(n_classes)(x)\n",
    "x = Activation(\"softmax\")(x)\n",
    "\n",
    "cnn4_flow = Model(inputs=input_holder, outputs=x)\n",
    "\n",
    "cnn4_flow.compile(\n",
    "    optimizer=\"Adamax\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "history4_4 = cnn4_flow.fit(\n",
    "    datagen.flow(x_train, y_train_ohe, batch_size=128),\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3,\n",
    "            start_from_epoch=50,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "cnn4_flow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a07f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the graph\n",
    "utils.plot_model(\n",
    "    cnn4,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"LR\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba = cnn4.predict(x_test)\n",
    "y_predict = np.round(y_predict_proba)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba_flow = cnn4_flow.predict(x_test)\n",
    "y_predict_flow = np.round(y_predict_proba_flow)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict_flow, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the loss over epochs\n",
    "epochs = history4.epoch\n",
    "loss = history4.history[\"loss\"]\n",
    "\n",
    "epochs_flow = history4_4.epoch\n",
    "loss_flow = history4_4.history[\"loss\"]\n",
    "\n",
    "# Plots the loss graph\n",
    "plt.plot(epochs, loss, label=\"CNN w/ Early Stopping\")\n",
    "plt.plot(epochs_flow, loss_flow, label=\"CNN (Flow Generator) w/ Early Stopping\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict, output_dict=True\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict_flow, output_dict=True, zero_division=0\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict_flow, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"CNN (With Flow Generator) Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 4 - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc03baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba_flow, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"CNN 4 (Flow Generator) - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e747013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history4.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 4\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e774d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(history4_4.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"CNN 4 With Flow Generator\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c759e9",
   "metadata": {},
   "source": [
    "# CNN 4 vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c173001",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn4,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 4\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn4_flow,\n",
    "    mlp,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 4 (Flow Generator)\",\n",
    "    title_2=\"MLP\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ab71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    cnn4,\n",
    "    cnn4_flow,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"CNN 4\",\n",
    "    title_2=\"CNN 4 (Flow Generator)\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff80bce",
   "metadata": {},
   "source": [
    "# GRADUATE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset images for compatibility with Resnet\n",
    "img_width, img_height = 64, 64\n",
    "img_color_mode = \"rgb\"\n",
    "classes = {0: \"NORMAL\", 1: \"PNEUMONIA\"}\n",
    "n_classes = 2\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"./train/\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    color_mode=img_color_mode,\n",
    "    batch_size=32,\n",
    "    image_size=(img_width, img_height),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"./test/\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    color_mode=img_color_mode,\n",
    "    batch_size=32,\n",
    "    image_size=(img_width, img_height),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "\n",
    "def normalize(image, label):\n",
    "    \"\"\"Normalize the pixel values of the image to be between 0 and 1.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(normalize)\n",
    "test_ds = test_ds.map(normalize)\n",
    "\n",
    "\n",
    "def process_ds_to_numpy(ds) -> tuple:\n",
    "    \"\"\"Returns the x, y numpy arrays from the dataset.\"\"\"\n",
    "    x, y = [], []\n",
    "    for image, label in ds.as_numpy_iterator():\n",
    "        x.append(np.array(image, dtype=np.float32))\n",
    "        y.append(np.array(label, dtype=np.int32))\n",
    "\n",
    "    return np.concatenate(x, axis=0), np.concatenate(y, axis=0)\n",
    "\n",
    "\n",
    "x_train, y_train = process_ds_to_numpy(train_ds)\n",
    "x_test, y_test = process_ds_to_numpy(test_ds)\n",
    "\n",
    "n_dimensions = x_train.shape[-1]\n",
    "\n",
    "datagen = preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    ")\n",
    "# Fits the data to the generator.\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# One-hot encodes the inputs\n",
    "y_train_ohe = utils.to_categorical(y_train, n_classes)\n",
    "y_test_ohe = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate our previous CNN for pre-training\n",
    "base_model = models.Sequential()\n",
    "base_model.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        input_shape=(img_width, img_height, n_dimensions),\n",
    "    )\n",
    ")\n",
    "\n",
    "base_model.add(Activation(\"relu\"))\n",
    "base_model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "    )\n",
    ")\n",
    "base_model.add(Activation(\"relu\"))\n",
    "base_model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adds 1 layer on flattened output\n",
    "base_model.add(Flatten())\n",
    "base_model.add(Dense(100, activation=\"relu\"))\n",
    "base_model.add(Dense(50, activation=\"relu\"))\n",
    "base_model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compiles the model\n",
    "base_model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "# Fits the model to training data\n",
    "base_model_history = base_model.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, start_from_epoch=20\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Save model for pre-training\n",
    "keras.models.save_model(base_model, \"base_model.h5\")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Prints model summary\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7678dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba = base_model.predict(x_test)\n",
    "y_predict = np.round(y_predict_proba)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5cf7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.models.load_model(\"base_model.h5\")\n",
    "\n",
    "# Set 1st layer to not trainable (e.g., \"Remove the top\")\n",
    "pretrained_model.layers[0].trainable = False\n",
    "\n",
    "# Implement transfer learning\n",
    "inputs = keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "x = pretrained_model.layers[1].output\n",
    "\n",
    "x = layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    "    input_shape=(img_width, img_height, n_dimensions),\n",
    ")(x)\n",
    "x = keras.layers.Activation(\"relu\")(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "outputs = keras.layers.Dense(2, activation=\"sigmoid\")(x)\n",
    "new_model = keras.Model(inputs=pretrained_model.input, outputs=outputs)\n",
    "\n",
    "new_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "new_model_history = new_model.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    batch_size=50,\n",
    "    epochs=100,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, start_from_epoch=25\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Displays model summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75695c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.models.load_model(\"base_model.h5\")\n",
    "\n",
    "# Set 1st layer to not trainable (e.g., \"Remove the top\")\n",
    "pretrained_model.layers[0].trainable = False\n",
    "\n",
    "# Implement transfer learning\n",
    "inputs = keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "x = pretrained_model.layers[1].output\n",
    "\n",
    "x = layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    data_format=\"channels_last\",\n",
    "    input_shape=(img_width, img_height, n_dimensions),\n",
    ")(x)\n",
    "x = keras.layers.Activation(\"relu\")(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "outputs = keras.layers.Dense(2, activation=\"sigmoid\")(x)\n",
    "new_model_flow = keras.Model(inputs=pretrained_model.input, outputs=outputs)\n",
    "\n",
    "new_model_flow.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "new_model_flow_history = new_model_flow.fit(\n",
    "    datagen.flow(x_train, y_train_ohe, batch_size=128),\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    batch_size=50,\n",
    "    epochs=100,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, start_from_epoch=25\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Displays model summary\n",
    "new_model_flow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba = new_model.predict(x_test)\n",
    "y_predict = np.round(y_predict_proba)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the y-prediction probability and y-prediction arrays\n",
    "y_predict_proba_flow = new_model_flow.predict(x_test)\n",
    "y_predict_flow = np.round(y_predict_proba_flow)\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(y_test_ohe, y_predict_flow, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the loss over epochs\n",
    "epochs = new_model_history.epoch\n",
    "loss = new_model_history.history[\"loss\"]\n",
    "\n",
    "epochs_flow = new_model_flow_history.epoch\n",
    "loss_flow = new_model_flow_history.history[\"loss\"]\n",
    "\n",
    "# Plots the loss graph\n",
    "plt.plot(epochs, loss, label=\"Transfer Learn CNN w/ Early Stopping\")\n",
    "plt.plot(epochs_flow, loss_flow, label=\"Transfer Learn CNN (Flow Generator) w/ Early Stopping\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict, output_dict=True\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"Transfer Learn CNN Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = metrics.classification_report(\n",
    "    y_test_ohe, y_predict_flow, output_dict=True, zero_division=0\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    metrics.confusion_matrix(\n",
    "        np.argmax(y_test_ohe, axis=1), np.argmax(y_predict_flow, axis=1)\n",
    "    ),\n",
    "    [classes[0], classes[1]],\n",
    "    normalize=False,\n",
    "    class_results=class_report[\"weighted avg\"],\n",
    "    title=\"Transfer Learn CNN (With Flow Generator) Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"Transfer Learn CNN - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for determining the ROC/AUC\n",
    "fpr, tpr, threshold = roc_curve(y_test, np.argmax(y_predict_proba_flow, axis=1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plots the ROC and AUC graph\n",
    "plt.title(\"Transfer Learn CNN (Flow Generator ) - Receiver Operating Characteristic\")\n",
    "plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(new_model_history.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"Transfer Learn CNN Model\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model history values\n",
    "hist_values = list(new_model_flow_history.history.values())\n",
    "\n",
    "# Variables for plotting the training versus testing\n",
    "train_loss   = hist_values[0]\n",
    "train_recall = hist_values[1]\n",
    "train_auc    = hist_values[2]\n",
    "val_loss     = hist_values[3]\n",
    "val_recall   = hist_values[4]\n",
    "val_auc      = hist_values[5]\n",
    "\n",
    "# Plots the training versus testing graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, (1, 2))\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_recall, label=\"Training Recall\")\n",
    "plt.plot(val_recall, label=\"Validation Recall\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_auc, label=\"Training AUC\")\n",
    "plt.plot(val_auc, label=\"Validation AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"Transfer Learn CNN With Flow Generator\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8ca6b",
   "metadata": {},
   "source": [
    "# CNN vs MLP: New Model vs Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17cdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    new_model,\n",
    "    base_model,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"New Model\",\n",
    "    title_2=\"Base Model\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    new_model_flow,\n",
    "    base_model,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"New Model (Flow Generator)\",\n",
    "    title_2=\"Base Model\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18dd48",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda48c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning using Resnet50\n",
    "res50_base = keras.applications.ResNet50(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(64, 64, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "res50_base.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(64, 64, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = res50_base(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(2, activation=\"sigmoid\")(x)\n",
    "res50 = keras.Model(inputs, outputs)\n",
    "\n",
    "res50.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "res50_history = res50.fit(\n",
    "    x_train,\n",
    "    y_train_ohe,\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    batch_size=50,\n",
    "    epochs=200,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, start_from_epoch=3\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "# Displays model summary\n",
    "res50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54464cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transfer learning with ResNet50\n",
    "res50_base = keras.applications.ResNet50(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(64, 64, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "res50_base.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(64, 64, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = res50_base(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(2)(x)\n",
    "res50_flow = keras.Model(inputs, outputs)\n",
    "\n",
    "res50_flow.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "res50_flow_history = res50_flow.fit(\n",
    "    datagen.flow(x_train, y_train_ohe, batch_size=50),\n",
    "    steps_per_epoch=len(x_train) // 128,\n",
    "    batch_size=50,\n",
    "    epochs=200,\n",
    "    validation_data=(x_test, y_test_ohe),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, start_from_epoch=20\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "clear_screen()\n",
    "\n",
    "res50_flow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec2f90",
   "metadata": {},
   "source": [
    "# CNN vs MLP: New Model (ResNet50) vs Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    res50,\n",
    "    base_model,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"New Model (ResNet 50)\",\n",
    "    title_2=\"Base Model\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb8325",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(\n",
    "    res50_flow,\n",
    "    base_model,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    title_1=\"New Model (ResNet 50) Flow Generator\",\n",
    "    title_2=\"Base Model\",\n",
    "    labels=[classes[0], classes[1]],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
